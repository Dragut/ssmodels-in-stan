# Filtering and Smoothing

## Filtering

From [@DurbinKoopman2012, Sec 4.3]


$$
\begin{aligned}[t]

 .
\end{aligned}
$$

Let $\vec{a}_t = \E(\vec{\alpha}_t | y_{1, \dots, t - 1})$ be the expected value
$\vec{P}_t = \Var(\vec{\alpha}_t | y_{1, \dots, t - 1})$ be the variance
of the state in $t + 1$ given data up to time $t$.
To calculate $\vec{\alpha}_{t + 1}$ and $\mat{P}_{t + 1}$ given the arrival of new
data at time $t$,
$$
\begin{aligned}[t]
\vec{v}_t &= \vec{y}_t - \mat{Z}_t \vec{a}_t - \vec{d}_t, \\
\mat{F}_t &= \mat{Z}_t \mat{P}_t \mat{Z}_t\T + \mat{H}_t, \\
\mat{K}_t &= \mat{T}_t \mat{P}_t \mat{Z}_t\T \mat{F}_t^{-1} \\
\vec{a}_{t + 1} &= \mat{T}_t \vec{a}_t + \mat{K}_t \vec{v}_t + \vec{c}_t \\
\mat{P}_{t + 1} &= \mat{T}_t \mat{P}_t (\mat{T}_t - \mat{K}_t \mat{Z}_t)\T + \mat{R}_t \mat{Q}_t \mat{R}_t\T .
\end{aligned}
$$
The vector $\vec{v}_t$ are the *one-step ahead forecast errors$,
and the matrix $\mat{K}_t$ is called the *Kalman gain*.

The filter can also be written to estimate the *filtered states*,
where $\vec{a}_{t|t} = \E(\vec{\alpha}_t | y_{1, \dots, t})$ is the expected value
and $\vec{P}_{t|t} = \Var(\vec{\alpha}_t | y_{1, \dots, t})$ is the variance of
the state $\vec{\alpha}_t$ given information up to *and including* $\vec{y}_t$.
The filter written this way is,
$$
\begin{aligned}[t]
\vec{v}_t &= \vec{y}_t - \mat{Z}_t \vec{a}_t - \vec{d}_t, \\
\mat{F}_t &= \mat{Z}_t \mat{P}_t \mat{Z}_t\T + \mat{H}_t, \\
\vec{a}_{t|t} &= \vec{a}_t + \mat{P}_t \mat{Z}_t\T \mat{F}_t^{-1} v_t , \\
\mat{P}_{t|t} &= \mat{P}_t - \mat{P}_t \mat{Z}_t\T \mat{F}_t^{-1} \mat{Z}_t \mat{P}_t , \\
\vec{a}_{t + 1} &= \mat{T}_{t} \vec{a}_{t|t} + \vec{c}_t, \\
\mat{P}_{t + 1} & = \mat{T}_t \mat{P}_{t|t} \mat{T}_t\T + \mat{R}_t \mat{Q}_t \mat{R}_t\T .
\end{aligned}
$$



matrix/vector         dimension
--------------------- --------------
$\vec{v}_t$           $p \times 1$
$\vec{a}_t$           $m \times 1$
$\vec{a}_{t|t}$       $m \times 1$
$\mat{F}_t$           $p \times p$
$\mat{K}_t$           $m \times p$
$\mat{P}_t$           $m \times m$
$\mat{P}_{t|T}$       $m \times m$
$\vec{x}_t$           $m \times 1$
$\mat{L}_t$           $m \times m$
--------------------- --------------

Table: Dimensions of matrices and vectors in the SSM

See [@DurbinKoopman2012, Sec 4.3.4]:
For a time-invariant state space model, the Kalman recursion for $\mat{P}_{t + 1}$ converges to a constant matrix $\bar{\mat{P}}$,
$$
\bar{\mat{P}} = \mat{T} \bar{\mat{P}} \mat{T}\T - \mat{T} \bar{\mat{P}} \mat{Z}\T \bar{\mat{F}}^{-1} \mat{Z} \bar{\mat{P}} \mat{T}\T + \mat{R} \mat{Q} \mat{R}\T ,
$$
where $\bar{\mat{F}} = \mat{Z} \bar{\mat{P}} \mat{Z}\T + \mat{H}$.

See [@DurbinKoopman2012, Sec 4.3.5]:
The *state estimation error* is,
$$
\vec{x}_t = \vec{\alpha}_t - \vec{a}_t,
$$
where $\Var(\vec{x}_t) = \mat{P}_t$.
The $v_t$ are sometimes called *innovations*, since they are the part of $\vec{y}_t$ not predicted from the past.
The innovation analog of the state space model is
$$
\begin{aligned}[t]
\vec{v}_t &= \mat{Z}_t \vec{x}_t + \vec{\varepsilon}_t ,  \\
\vec{x}_{t + 1} &= \mat{L} \vec{x}_{t} + \mat{R}_t \vec{\eta}_t - \mat{K}_t \vec{\varepsilon}_t , \\
\mat{K}_t &= \mat{T}_t \mat{P}_t \mat{Z}_t\T \mat{F}_t^{-1} , \\
\mat{L}_t &= \mat{T}_t - \mat{K}_t \mat{Z}_t ,
\mat{P}_{t + 1} &= \mat{T}_t \mat{P}_t \mat{L}_t\T +  \mat{R}_t \mat{Q}_t \mat{R}_T\T  .
\end{aligned}
$$
These recursions allow for a simpler derivation of $\mat{P}_{t + 1}$, and are useful for
the smoothing recursions.
Moreover, the one-step ahead forecast errors are indendendent, which allows for a simple derivation of the log-likelihood.

Alternative methods **TODO**

- square-root filtering
- precision filters
- sequential filtering

## Smoothing

Vector/Matrix                Dimension
---------------------------- -----------------------
$\vec{r}_t$                   $m \times 1$
$\vec{\vec{\alpha}}_t$        $m \times 1$
$\vec{u}_t$                   $p \times 1$
$\hat{\vec{\varepsilon}}_t$   $p \times 1$
$\hat{\vec{\eta}}_t$          $r \times 1$
$\mat{N}_t$                   $m \times m$
$\mat{V}_t$                   $m \times m$
$\mat{D}_t$                   $p \times p$
---------------------------- -----------------------

Table: Dimensions of vectors and matrices used in smoothing recursions

### State Smoothing

[@DurbinKoopman2012, Sec 4.4.2]

Smoothing calculates the estimate of the state using all observations, $\hat{\vec{\alpha}} = \E(\vec{\alpha}_t | \mat{Y}_n)$ and $\mat{V}_t = \Var(\vec{\alpha} | \mat{Y}_n)$.


Smoother in Eq 4.44. Let $\hat{\vec{a}} = \E(\alpha_t | \vec{y}_1, \dots \vec{y}_n)$ and $\mat{V}_t = \Var(\vec{\alpha}_t | \vec{y}_1, \dots \vec{y}_n)$, then
$$
\begin{aligned}[t]
\vec{r}_{t - 1} &= \mat{Z}_t\T \mat{F}_t^{-1} \vec{v}_t + \mat{L}_t\T \vec{r}_t , \\
\mat{N}_{t - 1} &= \mat{Z}_t\T \mat{F}_t^{-1} \mat{Z}_t + \mat{L}_t\T \mat{N}_t \mat{L}_t, \\
\hat{\vec{\alpha}}_t &= \vec{a}_t + \mat{P}_t \vec{r}_{t - 1} , \\
\mat{V}_t &= \mat{P}_t - \mat{P}_t \mat{N}_{t - 1} \mat{P}_t ,
\end{aligned}
$$
for $t = n, \dots, 1$, with $\vec{r}_n = \vec{0}$, and $\mat{N}_t = \mat{0}$.

During the filtering pass $\vec{v}_t$, $\mat{F}_t$, $\mat{K}_t$, and $\mat{P}_t$ for $t = 1, \dots, n$ need to be stored.
Alternatively, $\vec{a}_t$ and $\mat{P}_t$ only can be stored, and $\vec{v}_t$, $\mat{F}_t$, $\mat{K}_t$ recalculated on the fly.
However, since the dimensions of $\vec{v}_t$, $\mat{F}_t$, $\mat{K}_t$ are usually small relative to $\vec{a}_t$ and $\mat{P}_t$ is is usually worth storing them.



### Updating Smoothed States

[@DurbinKoopman2012, Sec 4.4.5]

If a new observation arises, existing smoothed estimates of the states can be updated without
having to run the full state smoother again.

Suppose a new obervation, $\vec{y}_{n + 1}$, is available.
We want to calculate $\hat{\vec{\alpha}}_{t|n + 1} = \E(\vec{\alpha} | \mat{Y}_{n + 1})$ and $\mat{V}_{t|n + 1} = \Var(\vec{\alpha}) | \mat{Y}_{n + 1}$, when we already have  $\hat{\vec{\alpha}}_{t|n} = \E(\vec{\alpha} | \mat{Y}_{n})$ and $\mat{V}_{t|n} = \Var(\vec{\alpha} | \mat{Y}_{n})$ from running smoother when $\mat{Y}_n$ was available.

Let $b_{t|n + 1} = \mat{L}_t\T \cdots \mat{L}_n\T$ with $\vec{b}_{t|n + 1} = \mat{I}_m$.
Then $b_{t|n + 1} = \mat{L}_t\T b_{t+1|n+1}$ for $t = n, \dots, 1$.
Then the states can be updated as,
$$
\begin{aligned}[t]
\hat{\vec{\alpha}}_{t|n + 1} &= \hat{\vec{a}}_{t|n} + \mat{P}_t \vec{b}_{t|n + 1} \mat{Z}_{n + 1}\T \mat{F}_{n + 1}^{-1} \vec{v}_{n + 1} , \\
\mat{V}_{t|n + 1} &= \mat{V}_{t|n} - \mat{P}_t \vec{b}_{t|n + 1} \mat{Z}_{n + 1}\T \mat{F}_{n + 1}^{-1} \mat{Z}_{n + 1} \vec{b}_{t|n + 1}\T \mat{P}_t ,
\end{aligned}
$$
for $n = t, t + 1, \dots$, with
$$
\begin{aligned}[t]
\hat{\vec{\alpha}}_{n|n} &= \vec{a}_n + \mat{P}_n \mat{Z}_n\T \mat{F}_n^{-1} \vec{v}_n, \\
\mat{V}_{n|n} &= \mat{P}_n - \mat{P}_n \mat{Z}_n\T \mat{F}_n^{-1} \mat{Z}_n \mat{P}_n ,
\end{aligned}
$$
The values of $\mat{P}_t$, $\mat{L}_t$, $\mat{F}_{n + 1}$, and $\vec{v}_{n + 1}$ are available from the Kalman filter.


### Disturbance smoothing

[@DurbinKoopman2012, Sec 4.5]

Disturbance smoothing calculates the smoothed estimates, $\hat{\vec{\varepsilon}}_t = \E(\vec{\varepsilon} | \mat{Y}_n)$ and $\hat{\vec{\eta}} = \E(\vec{\eta}_t | \mat{Y}_n)$.

$$
\begin{aligned}[t]
\hat{\vec{\varepsilon}}_t &= \mat{H}_t (\mat{F}^{-1} \vec{v}_t - \mat{K}_t\T \vec{r}_t) , &
\Var(\vec{\varepsilon}_t | \mat{Y}_n) &= \mat{H}_t - \mat{H}_t (\mat{F}_t^{-1} + \mat{K}_t\T \mat{N}_t \mat{K}_t) \mat{H}_t , \\
\hat{\vec{\eta}}_t &= \mat{Q}_t \mat{R}_t\T \vec{r}_t , &
\Var(\vec{\eta}_t | \mat{Y}_n) &= \mat{Q}_t - \mat{Q}_t \mat{R}_t\T \mat{N}_t \mat{R}_t \mat{Q}_t , \\
\vec{r}_{t - 1} &= \mat{Z}_t\T \mat{F}_t^{-1} \vec{v}_t + \mat{L}_t\T \vec{r}_t , &
\mat{N}_{t - 1} &= \mat{Z}_t\T \mat{F}_t^{-1} \mat{Z}_t + \mat{L}_t\T \mat{N}_t \mat{L}_t
\end{aligned}
$$


Alternatively, these equations can be rewritten as,
$$
\begin{aligned}[t]
\hat{\vec{\varepsilon}}_t &= \mat{H}_t \vec{u}_t , &
\Var(\vec{\varepsilon}_t | \mat{Y}_n) &= \mat{H}_t - \mat{H}_t \mat{D}_t \mat{H}_t , \\
\hat{\vec{\eta}}_t &= \mat{Q}_t \mat{R}_t\T \vec{r}_t , &
\Var(\vec{\eta}_t | \mat{Y}_n) &= \mat{Q}_t - \mat{Q}_t \mat{R}_t\T \mat{N}_t \mat{R}_t \mat{Q}_t , \\
\vec{u}_t &= \mat{F}^{-1} \vec{v}_t - \mat{K}_t\T \vec{r}_t , &
\mat{D}_t &= \mat{F}_t^{-1} + \mat{K}_t\T \mat{N}_t \mat{K}_t , \\
\vec{r}_{t - 1} &= \mat{Z}_t\T \vec{u}_t + \mat{T}_t\T \vec{r}_t , &
\mat{N}_{t - 1} &= \mat{Z}_t\T \mat{D}_t \mat{Z}_t + \mat{T}_t\T \mat{N}_t \mat{T}_t - \mat{Z}_t\T \mat{K}_t\T \mat{N}_t \mat{T}_t - \mat{T}_t\T \mat{N}_t \mat{K}_t \mat{Z}_t .
\end{aligned}
$$
This reformulation can be computationally useful since it relies on the system matrices $\mat{Z}_t$ and $\mat{T}_t$ which are often sparse.

The smoothing error $\vec{u}_t$ and $\vec{r}_t$ are important in their own right.
The vector $\vec{r}_t$ is the scaled smoothed estimator of $\vec{\eta}_t$.

The disturbance smoothing recursions require only $\vec{v}_t$, $\mat{f}_t$, and $\mat{K}_t$ from the Kalman filter.
Unlike the state smoother, they do not require the vector $\vec{a}_t$ or matrix $\mat{P}_t$.


### Fast state smoothing

[@DurbinKoopman2012, Sec 4.6.2]

If the variances of the states do not need to be calculated, then a faster smoothing algorithm
can be used (Koopman 1993).
The fast state smoother is defiend as,
$$
\hat{\vec{\alpha}}_t = \mat{T}_t \hat{\vec{\alpha}}_t + \mat{R}_t \mat{Q}_t \mat{R}_t\T \vec{r}_t ,
$$
for $t = 1, \dots, n$.
This is initialized via,
$$
\hat{\vec{\alpha}}_1 = \vec{a}_1 + \mat{P}_1 \vec{r}_0 .
$$
Thus the smoother is run, but only calculating $\vec{r}_n, \dots, \vec{r}_0$.
Then, $\hat{\vec{\alpha}}_1, \dots \hat{\vec{\alpha}}_t$ are calculated using the above equation.

### Classical state smoothing

See [@DurbinKoopman2012, Sec 4.6.1]

The original state smoothing algorithm from Anderson and Moore (1979) is
$$
\begin{aligned}[t]
\hat{\vec{\alpha}}_t &= \vec{a}_{t|t} + \mat{P}_{t|t} \mat{T}_t\T \mat{P}_{t + 1}^{-1}(\hat{\vec{\alpha}}_{t + 1} - \vec{a}_{t + 1}) .
\end{aligned}
$$
Note that $\mat{T} \mat{P}_{t|t} = \mat{L}_t \mat{P}_t$.
Relative to the other state smoothing algorithm the classical smoother is more computationally burdensome because it requires inverting $\mat{P}_{t + 1}$.
The other state smoother only requires inverting $\mat{F}_t$, but that has already been inverted in the filtering pass.

## Simulation smoothers

Simulation smoothing draws samples of the states, $p(\vec{\alpha}_1, \dots, \vec{\alpha}_n | \vec{y}_{1:n})$, or disturbances, $p(\vec{\varepsilon}_1, \dots, \vec{\varepsilon}_n | \vec{y}_{1:n})$ and $p(\vec{\eta}_1, \dots, \vec{\eta}_n | \vec{y}_{1:n})$.

### Mean corrections

[@DurbinKoopman2012, Sec 4.9]

### Disturbances

Let $\vec{w} = (\vec{\varepsilon}_1', \var{\eta}_1', \dots, \vec{\varepsilon}_n', \vec{\eta}_n')$ and let $\hat{\vec{w}} = \E(\vec{w} | \vec{y}_{1:n})$.
We want to sample from $p(\vec{w} | \vec{y}_{1:n})$.

1. Run filter and smoother to calculate $\hat{\vec{w}}$ which consists of the smoothed observation disturbances $\E(\vec{\varepsilon}_{1:n})$ and state disturbances $\E(\vec{\eta}_{1:n})$.
2. Draw samples from the unconditional disturbances of $\vec{w}$,
    $$
    \begin{aligned}[t]
    \vec{w} &\sim N(0, \Phi) &\Phi &= \ndiag(H_1, Q_1, \dots, H_n, Q_n) .
    \end{aligned}
    $$
3. Simulate from the state space system using $\vec{w}$ to simulate observations $\vec{y}^+$.
4. Run a filter and smoother on $\vec{y}^+$ to calculate $\E(\vec{varepsilon}_{1:n} | \vec{y}^+_{1:n})$ and $\E(\vec{eta}_{1:n} | \vec{y}^+_{1:n})$, and call the vector of these expected disturbances $\hat{\vec{w}}^+{1:n}$.
5. A sample from $p(\vec{w} | \vec{y}_{1:n})$ is
    $$
    \tilde{\vec{w}} = \vec{w}^+ - \hat{\vec{w}}^+ + \hat{\vec{w}} .
    $$


### States

**TODO**

### de Jong-Shephard method


While the mean-corrections method usually works, it may fail in some cases due
to imposed ill-defined variance matrices (see Jungbacker and Koopman 2007, sec 1).
These recursions were developed in de Jong and Shephard (1995),

**TODO**

### Forward-Filter Backwards smoother

**TODO**

## Missing observations

[@DurbinKoopman2012, Sec 4.10]

When all observations at time $t$ are missing, the filtering recursions become,
$$
\begin{aligned}[t]
\vec{a}_{t|t} &= \vec{a}_t , \\
\mat{P}_{t|t} &= \mat{P}_t , \\
\vec{a}_{t + 1} &= \mat{T}_t \vec{a}_t + \vec{c}_t \\
\mat{P}_{t + 1} &= \mat{T}_t \mat{P}_t \mat{T}_t\T + \mat{R}_t \mat{Q}_t \mat{R}_t\T
\end{aligned}
$$
This is equivalent to setting $\mat{Z}_t = \mat{0}$ (implying also that $\mat{K}_t = \mat{0}$) in the filtering equations.
For smoothing, also replace $\mat{Z}_t = \mat{0}$,
$$
\begin{aligned}[t]
\vec{r}_{t - 1} &= \mat{T}_t\T \vec{r}_t , \\
\mat{N}_{t - 1} &= \mat{T}_t\T \mat{N}_t \mat{T}_t,
\end{aligned}
$$

When some, but not all observations are missing, then replace the observation equation by,
$$
\begin{aligned}[t]
\vec{y}^*_t &= \mat{Z}^*_t \vec{\alpha}_t + \vec{\varepsilon}_t^*, & \vec{\varepsilon}_t^* &\sim N(\vec{0}, \mat{H}_t^*),
\end{aligned}
$$
where,
$$
\begin{aligned}[t]
\vec{y}^*_t &= \mat{W}_t \vec{y}_t, \\
\mat{Z}^* &= \mat{W}_t \mat{Z}_t , \\
\vec{\varepsilon}_t &= \mat{W}_t \vec{\varepsilon}_t , \\
\mat{H}^*_t &= \mat{W}_t \mat{H}_t \mat{W}_t\T ,
\end{aligned}
$$
and $\mat{W}_t$ is a selection matrix to select non-missing values.
In smoothing the missing elements are estimated by the appropriate elements of $\mat{Z}_t \hat{\vec{alpha}}_t$, where $\hat{\vec{\alpha}}_t$ is the smoothed state.


## Forecasting matrices

[@DurbinKoopman2012, Sec 4.11]

Forecasting future observations are the same as treating the future observations as missing,
$$
\begin{aligned}[t]
\bar{\vec{y}}_{n + j} &= \mat{Z}_{n + j} \bar{\vec{a}}_{n + j} \\
\bar{\mat{F}}_{n + j} &= \mat{Z}_{n + j} \bar{\mat{P}}_{n + j} \mat{Z}_{n + j}\T + \mat{H}_{n + j} .
\end{aligned}
$$
