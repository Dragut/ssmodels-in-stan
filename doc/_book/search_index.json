[
["index.html", "State Space Models in Stan Chapter 1 Introduction", " State Space Models in Stan Jeffrey B. Arnold 2016-06-18 Chapter 1 Introduction This contains documentation for “State Space Models in Stan” "],
["the-linear-state-space-model.html", "Chapter 2 The Linear State Space Model", " Chapter 2 The Linear State Space Model (Durbin and Koopman 2012, Sec 3.1) The linear Gaussian state space model (SSM)1 the the \\(n\\)-dimensional observation sequence $1, , \\[ \\begin{aligned}[t] \\vec{y}_t &amp;= \\vec{d}_t + \\mat{Z}_t \\vec{\\alpha}_t + \\vec{\\varepsilon}_t, &amp; \\vec{\\varepsilon}_t &amp; \\sim N(0, \\mat{H}_t), \\\\ \\vec{\\alpha}_{t + 1} &amp;= \\vec{c}_t + \\mat{T}_t \\vec{\\alpha}_t + \\mat{R}_t \\vec{\\eta}_t, &amp; \\vec{\\eta}_t &amp; \\sim N(0, \\mat{Q}_t), \\\\ &amp;&amp; \\vec{\\alpha}_1 &amp;\\sim N(\\vec{a}_1, \\mat{P}_1) \\end{aligned} \\] for \\(t = 1, \\dots, n\\). The first equation is called the observation or measurement equation. The second equation is called the state, transition, or system equation. The vector \\(\\vec{y}_t\\) is a \\(p \\times 1\\) vector called the observation vector. The vector \\(\\alpha{\\alpha}_t\\) is a \\(m \\times 1\\) vector called the state vector. The matrices are vectors, \\(\\mat{Z}_t\\),\\(\\mat{T}_t\\), \\(\\mat{R}_t\\), \\(\\mat{H}_t\\), \\(\\mat{Q}_t\\), \\(c_t\\), and \\(d_t\\) are called the system matrices. The system matrices are considered fixed and known in the filtering and smoothing equations below, but can be parameters themselves. The \\(p \\times m\\) matrix \\(\\mat{Z}_t\\) links the observation vector \\(\\vec{y}_t\\) with the state vector \\(\\vec{\\alpha}_t\\). The \\(m \\times m\\) transition matrix \\(\\mat{T}_t\\) determines the evolution of the state vector, \\(\\vec{\\alpha}_t\\). The \\(r \\times 1\\) vector \\(\\vec{\\eta}_t\\) is called the state disturbance vector, and the \\(p \\times 1\\) vector \\(\\vec{\\varepsilon}_t\\) is called the observation disturbance vector. An assumption is that the state and observation disturbance vectors are uncorrelated, \\(\\Cov(\\vec{\\varepsilon}_t, \\vec{\\eta}_t) = 0\\). In a general state space model, the normality assumptions of the densities of \\(\\vec{\\varepsilon}\\) and \\(\\vec{\\eta}\\) are dropped. In many cases \\(\\mat{R}_t\\) is the identity matrix. It is possible to define \\(\\eta^*_t = \\mat{R}_t \\vec{\\eta}_t\\), and \\(\\mat{Q}^* = \\mat{R}_t \\mat{Q}_t&#39; \\mat{R}&#39;_t\\). However, if \\(\\mat{R}_t\\) is \\(m \\times r\\) and \\(r &lt; m\\), and \\(\\mat{Q}_t\\) is nonsingular, then it is useful to work with the nonsingular \\(\\vec{\\eta}_t\\) rather than a singular \\(\\vec{\\eta}_t^*\\). The initial state vector \\(\\vec{\\alpha}_1\\) is assume to be generated as, \\[ \\alpha_1 \\sim N(\\vec{a}_1, \\mat{P}_1) \\] independently of the observation and state disturbances \\(\\vec{\\varepsilon}\\) and \\(\\vec{\\eta}\\). The values of \\(\\vec{a}_1\\) and \\(\\mat{P}_1\\) can be considered as given and known in most stationary processes. When the process is nonstationary, the elements of \\(\\vec{a}_1\\) need to be treated as unknown and estimated. This is called initialization. Dimensions of matrices and vectors in the SSM matrix/vector dimension \\(\\vec{y}_t\\) \\(p \\times 1\\) \\(\\vec{\\alpha}_t\\) \\(m \\times 1\\) \\(\\vec{\\varepsilon}_t\\) \\(m \\times 1\\) \\(\\vec{\\eta}_t\\) \\(r \\times 1\\) \\(\\vec{a}_1\\) \\(m \\times 1\\) \\(\\vec{c}_t\\) \\(m \\times 1\\) \\(\\vec{d}_t\\) \\(p \\times 1\\) \\(\\mat{Z}_t\\) \\(p \\times m\\) \\(\\mat{T}_t\\) \\(m \\times m\\) \\(\\mat{H}_t\\) \\(p \\times p\\) \\(\\mat{R}_t\\) \\(m \\times r\\) \\(\\mat{Q}_t\\) \\(r \\times r\\) \\(\\mat{P}_1\\) \\(m \\times m\\) References "],
["filtering-and-smoothing.html", "Chapter 3 Filtering and Smoothing ", " Chapter 3 Filtering and Smoothing "],
["filtering.html", "3.1 Filtering", " 3.1 Filtering From (Durbin and Koopman 2012, Sec 4.3) \\[ \\begin{aligned}[t] \\vec{v}_t &amp;= \\vec{y}_t - \\mat{Z}_t \\vec{a}_t - \\vec{d}_t, \\\\ \\mat{F}_t &amp;= \\mat{Z}_t \\mat{P}_t \\mat{Z}_t\\T + \\mat{H}_t, \\\\ \\vec{a}_{t|t} &amp;= \\vec{a}_t + \\mat{P}_t \\mat{Z}_t\\T \\mat{F}_t^{-1} v_t , \\\\ \\mat{P}_{t|t} &amp;= \\mat{P}_t - \\mat{P}_t \\mat{Z}_t\\T \\mat{F}_t^{-1} \\mat{Z}_t \\mat{P}_t , \\\\ \\vec{a}_{t + 1} &amp;= \\mat{T}_t \\vec{a}_t + \\mat{K}_t \\vec{v}_t + \\vec{c}_t \\\\ &amp;= \\mat{T}_{t} \\vec{a}_{t|t} + \\vec{c}_t, \\\\ \\mat{P}_{t + 1} &amp;= \\mat{T}_t \\mat{P}_t (\\mat{T}_t - \\mat{K}_t \\mat{Z}_t)\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T \\\\ &amp; = \\mat{T}_t \\mat{P}_{t|t} \\mat{T}_t\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T \\end{aligned} \\] The vector \\(\\vec{v}_t\\) are the *one-step ahead forecast errors$. The matrix \\(\\mat{K}_t\\) is called the Kalman gain, \\[ \\mat{K}_t = \\mat{T}_t \\mat{P}_t \\mat{Z}_t\\T \\mat{F}_t^{-1} . \\] This gives, \\[ \\begin{aligned}[t] \\vec{a}_{t + 1} &amp;= \\mat{T} \\vec{a}_{t|t} = \\mat{T}_t \\vec{a}_t + \\mat{K}_t \\vec{v}_t , \\\\ \\vec{P}_{t + 1} &amp;= \\mat{T}_t \\mat{P}_t \\left( \\mat{T}_t - \\mat{K}_t \\mat{Z}_t \\right)\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T . \\end{aligned} \\] These two equations are called the prediction step. The steps for \\(\\vec{a}_{t|t}\\) and \\(\\mat{P}_{t|t}\\) are called the updating step. \\[ \\begin{aligned}[t] \\vec{a}_t &amp;= \\E(\\vec{\\alpha}_t | y_{1, \\dots, t - 1}), &amp; \\vec{P}_t &amp;= \\Var(\\vec{\\alpha}_t | y_{1, \\dots, t - 1}), \\\\ \\vec{a}_{t|t} &amp;= \\E(\\vec{\\alpha}_t | y_{1, \\dots, t}), &amp; \\vec{P}_{t|t} &amp;= \\Var(\\vec{\\alpha}_t | y_{1, \\dots, t}) . \\end{aligned} \\] Dimensions of matrices and vectors in the SSM matrix/vector dimension \\(\\vec{v}_t\\) \\(p \\times 1\\) \\(\\vec{a}_t\\) \\(m \\times 1\\) \\(\\vec{a}_{t|t}\\) \\(m \\times 1\\) \\(\\mat{F}_t\\) \\(p \\times p\\) \\(\\mat{K}_t\\) \\(m \\times p\\) \\(\\mat{P}_t\\) \\(m \\times m\\) \\(\\mat{P}_{t|T}\\) \\(m \\times m\\) \\(\\vec{x}_t\\) \\(m \\times 1\\) \\(\\mat{L}_t\\) \\(m \\times m\\) See (Durbin and Koopman 2012, Sec 4.3.4): For a time-invariant state space model, the Kalman recursion for \\(\\mat{P}_{t + 1}\\) converges to a constant matrix \\(\\bar{\\mat{P}}\\), \\[ \\bar{\\mat{P}} = \\mat{T} \\bar{\\mat{P}} \\mat{T}\\T - \\mat{T} \\bar{\\mat{P}} \\mat{Z}\\T \\bar{\\mat{F}}^{-1} \\mat{Z} \\bar{\\mat{P}} \\mat{T}\\T + \\mat{R} \\mat{Q} \\mat{R}\\T , \\] where \\(\\bar{\\mat{F}} = \\mat{Z} \\bar{\\mat{P}} \\mat{Z}\\T + \\mat{H}\\). See (Durbin and Koopman 2012, Sec 4.3.5): The state estimation error is, \\[ \\vec{x}_t = \\vec{\\alpha}_t - \\vec{a}_t, \\] where \\(\\Var(\\vec{x}_t) = \\mat{P}_t\\). The \\(v_t\\) are sometimes called innovations, since they are the part of \\(\\vec{y}_t\\) not predicted from the past. The innovation analog of the state space model is \\[ \\begin{aligned}[t] \\vec{v}_t &amp;= \\mat{Z}_t \\vec{x}_t + \\vec{\\varepsilon}_t , \\\\ \\vec{x}_{t + 1} &amp;= \\mat{L} \\vec{x}_{t} + \\mat{R}_t \\vec{\\eta}_t - \\mat{K}_t \\vec{\\varepsilon}_t , \\\\ \\mat{K}_t &amp;= \\mat{T}_t \\mat{P}_t \\mat{Z}_t\\T \\mat{F}_t^{-1} , \\\\ \\mat{L}_t &amp;= \\mat{T}_t - \\mat{K}_t \\mat{Z}_t , \\mat{P}_{t + 1} &amp;= \\mat{T}_t \\mat{P}_t \\mat{L}_t\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_T\\T . \\end{aligned} \\] These recursions allow for a simpler derivation of \\(\\mat{P}_{t + 1}\\), and are useful for the smoothing recursions. Moreover, the one-step ahead forecast errors are indendendent, which allows for a simple derivation of the log-likelihood. Alternative methods TODO square-root filtering precision filters sequential filtering References "],
["smoothing.html", "3.2 Smoothing", " 3.2 Smoothing Dimensions of vectors and matrices used in smoothing recursions Vector/Matrix Dimension \\(\\vec{r}_t\\) \\(m \\times 1\\) \\(\\vec{\\vec{\\alpha}}_t\\) \\(m \\times 1\\) \\(\\vec{u}_t\\) \\(p \\times 1\\) \\(\\hat{\\vec{\\varepsilon}}_t\\) \\(p \\times 1\\) \\(\\hat{\\vec{\\eta}}_t\\) \\(r \\times 1\\) \\(\\mat{N}_t\\) \\(m \\times m\\) \\(\\mat{V}_t\\) \\(m \\times m\\) \\(\\mat{D}_t\\) \\(p \\times p\\) 3.2.1 State Smoothing (Durbin and Koopman 2012, Sec 4.4.2) Smoothing calculates the estimate of the state using all observations, \\(\\hat{\\vec{\\alpha}} = \\E(\\vec{\\alpha}_t | \\mat{Y}_n)\\) and \\(\\mat{V}_t = \\Var(\\vec{\\alpha} | \\mat{Y}_n)\\). Smoother in Eq 4.44. Let \\(\\hat{\\vec{a}} = \\E(\\alpha_t | \\vec{y}_1, \\dots \\vec{y}_n)\\) and \\(\\mat{V}_t = \\Var(\\vec{\\alpha}_t | \\vec{y}_1, \\dots \\vec{y}_n)\\), then \\[ \\begin{aligned}[t] \\vec{r}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{F}_t^{-1} \\vec{v}_t + \\mat{L}_t\\T \\vec{r}_t , \\\\ \\mat{N}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{F}_t^{-1} \\mat{Z}_t + \\mat{L}_t\\T \\mat{N}_t \\mat{L}_t, \\\\ \\hat{\\vec{\\alpha}}_t &amp;= \\vec{a}_t + \\mat{P}_t \\vec{r}_{t - 1} , \\\\ \\mat{V}_t &amp;= \\mat{P}_t - \\mat{P}_t \\mat{N}_{t - 1} \\mat{P}_t , \\end{aligned} \\] for \\(t = n, \\dots, 1\\), with \\(\\vec{r}_n = \\vec{0}\\), and \\(\\mat{N}_t = \\mat{0}\\). During the filtering pass \\(\\vec{v}_t\\), \\(\\mat{F}_t\\), \\(\\mat{K}_t\\), and \\(\\mat{P}_t\\) for \\(t = 1, \\dots, n\\) need to be stored. Alternatively, \\(\\vec{a}_t\\) and \\(\\mat{P}_t\\) only can be stored, and \\(\\vec{v}_t\\), \\(\\mat{F}_t\\), \\(\\mat{K}_t\\) recalculated on the fly. However, since the dimensions of \\(\\vec{v}_t\\), \\(\\mat{F}_t\\), \\(\\mat{K}_t\\) are usually small relative to \\(\\vec{a}_t\\) and \\(\\mat{P}_t\\) is is usually worth storing them. 3.2.2 Updating Smoothed States (Durbin and Koopman 2012, Sec 4.4.5) If a new observation arises, existing smoothed estimates of the states can be updated without having to run the full state smoother again. Suppose a new obervation, \\(\\vec{y}_{n + 1}\\), is available. We want to calculate \\(\\hat{\\vec{\\alpha}}_{t|n + 1} = \\E(\\vec{\\alpha} | \\mat{Y}_{n + 1})\\) and \\(\\mat{V}_{t|n + 1} = \\Var(\\vec{\\alpha}) | \\mat{Y}_{n + 1}\\), when we already have \\(\\hat{\\vec{\\alpha}}_{t|n} = \\E(\\vec{\\alpha} | \\mat{Y}_{n})\\) and \\(\\mat{V}_{t|n} = \\Var(\\vec{\\alpha} | \\mat{Y}_{n})\\) from running smoother when \\(\\mat{Y}_n\\) was available. Let \\(b_{t|n + 1} = \\mat{L}_t\\T \\cdots \\mat{L}_n\\T\\) with \\(\\vec{b}_{t|n + 1} = \\mat{I}_m\\). Then \\(b_{t|n + 1} = \\mat{L}_t\\T b_{t+1|n+1}\\) for \\(t = n, \\dots, 1\\). Then the states can be updated as, \\[ \\begin{aligned}[t] \\hat{\\vec{\\alpha}}_{t|n + 1} &amp;= \\hat{\\vec{a}}_{t|n} + \\mat{P}_t \\vec{b}_{t|n + 1} \\mat{Z}_{n + 1}\\T \\mat{F}_{n + 1}^{-1} \\vec{v}_{n + 1} , \\\\ \\mat{V}_{t|n + 1} &amp;= \\mat{V}_{t|n} - \\mat{P}_t \\vec{b}_{t|n + 1} \\mat{Z}_{n + 1}\\T \\mat{F}_{n + 1}^{-1} \\mat{Z}_{n + 1} \\vec{b}_{t|n + 1}\\T \\mat{P}_t , \\end{aligned} \\] for \\(n = t, t + 1, \\dots\\), with \\[ \\begin{aligned}[t] \\hat{\\vec{\\alpha}}_{n|n} &amp;= \\vec{a}_n + \\mat{P}_n \\mat{Z}_n\\T \\mat{F}_n^{-1} \\vec{v}_n, \\\\ \\mat{V}_{n|n} &amp;= \\mat{P}_n - \\mat{P}_n \\mat{Z}_n\\T \\mat{F}_n^{-1} \\mat{Z}_n \\mat{P}_n , \\end{aligned} \\] The values of \\(\\mat{P}_t\\), \\(\\mat{L}_t\\), \\(\\mat{F}_{n + 1}\\), and \\(\\vec{v}_{n + 1}\\) are available from the Kalman filter. 3.2.3 Disturbance smoothing (Durbin and Koopman 2012, Sec 4.5) Disturbance smoothing calculates the smoothed estimates, \\(\\hat{\\vec{\\varepsilon}}_t = \\E(\\vec{\\varepsilon} | \\mat{Y}_n)\\) and \\(\\hat{\\vec{\\eta}} = \\E(\\vec{\\eta}_t | \\mat{Y}_n)\\). \\[ \\begin{aligned}[t] \\hat{\\vec{\\varepsilon}}_t &amp;= \\mat{H}_t (\\mat{F}^{-1} \\vec{v}_t - \\mat{K}_t\\T \\vec{r}_t) , &amp; \\Var(\\vec{\\varepsilon}_t | \\mat{Y}_n) &amp;= \\mat{H}_t - \\mat{H}_t (\\mat{F}_t^{-1} + \\mat{K}_t\\T \\mat{N}_t \\mat{K}_t) \\mat{H}_t , \\\\ \\hat{\\vec{\\eta}}_t &amp;= \\mat{Q}_t \\mat{R}_t\\T \\vec{r}_t , &amp; \\Var(\\vec{\\eta}_t | \\mat{Y}_n) &amp;= \\mat{Q}_t - \\mat{Q}_t \\mat{R}_t\\T \\mat{N}_t \\mat{R}_t \\mat{Q}_t , \\\\ \\vec{r}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{F}_t^{-1} \\vec{v}_t + \\mat{L}_t\\T \\vec{r}_t , &amp; \\mat{N}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{F}_t^{-1} \\mat{Z}_t + \\mat{L}_t\\T \\mat{N}_t \\mat{L}_t \\end{aligned} \\] Alternatively, these equations can be rewritten as, \\[ \\begin{aligned}[t] \\hat{\\vec{\\varepsilon}}_t &amp;= \\mat{H}_t \\vec{u}_t , &amp; \\Var(\\vec{\\varepsilon}_t | \\mat{Y}_n) &amp;= \\mat{H}_t - \\mat{H}_t \\mat{D}_t \\mat{H}_t , \\\\ \\hat{\\vec{\\eta}}_t &amp;= \\mat{Q}_t \\mat{R}_t\\T \\vec{r}_t , &amp; \\Var(\\vec{\\eta}_t | \\mat{Y}_n) &amp;= \\mat{Q}_t - \\mat{Q}_t \\mat{R}_t\\T \\mat{N}_t \\mat{R}_t \\mat{Q}_t , \\\\ \\vec{u}_t &amp;= \\mat{F}^{-1} \\vec{v}_t - \\mat{K}_t\\T \\vec{r}_t , &amp; \\mat{D}_t &amp;= \\mat{F}_t^{-1} + \\mat{K}_t\\T \\mat{N}_t \\mat{K}_t , \\\\ \\vec{r}_{t - 1} &amp;= \\mat{Z}_t\\T \\vec{u}_t + \\mat{T}_t\\T \\vec{r}_t , &amp; \\mat{N}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{D}_t \\mat{Z}_t + \\mat{T}_t\\T \\mat{N}_t \\mat{T}_t - \\mat{Z}_t\\T \\mat{K}_t\\T \\mat{N}_t \\mat{T}_t - \\mat{T}_t\\T \\mat{N}_t \\mat{K}_t \\mat{Z}_t . \\end{aligned} \\] This reformulation can be computationally useful since it relies on the system matrices \\(\\mat{Z}_t\\) and \\(\\mat{T}_t\\) which are often sparse. The smoothing error \\(\\vec{u}_t\\) and \\(\\vec{r}_t\\) are important in their own right. The vector \\(\\vec{r}_t\\) is the scaled smoothed estimator of \\(\\vec{\\eta}_t\\). The disturbance smoothing recursions require only \\(\\vec{v}_t\\), \\(\\mat{f}_t\\), and \\(\\mat{K}_t\\) from the Kalman filter. Unlike the state smoother, they do not require the vector \\(\\vec{a}_t\\) or matrix \\(\\mat{P}_t\\). 3.2.4 Fast state smoothing (Durbin and Koopman 2012, Sec 4.6.2) If the variances of the states do not need to be calculated, then a faster smoothing algorithm can be used (Koopman 1993). The fast state smoother is defiend as, \\[ \\hat{\\vec{\\alpha}}_t = \\mat{T}_t \\hat{\\vec{\\alpha}}_t + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T \\vec{r}_t , \\] for \\(t = 1, \\dots, n\\). This is initialized via, \\[ \\hat{\\vec{\\alpha}}_1 = \\vec{a}_1 + \\mat{P}_1 \\vec{r}_0 . \\] Thus the smoother is run, but only calculating \\(\\vec{r}_n, \\dots, \\vec{r}_0\\). Then, \\(\\hat{\\vec{\\alpha}}_1, \\dots \\hat{\\vec{\\alpha}}_t\\) are calculated using the above equation. 3.2.5 Classical state smoothing See (Durbin and Koopman 2012, Sec 4.6.1) The original state smoothing algorithm from Anderson and Moore (1979) is \\[ \\begin{aligned}[t] \\hat{\\vec{\\alpha}}_t &amp;= \\vec{a}_{t|t} + \\mat{P}_{t|t} \\mat{T}_t\\T \\mat{P}_{t + 1}^{-1}(\\hat{\\vec{\\alpha}}_{t + 1} - \\vec{a}_{t + 1}) . \\end{aligned} \\] Note that \\(\\mat{T} \\mat{P}_{t|t} = \\mat{L}_t \\mat{P}_t\\). Relative to the other state smoothing algorithm the classical smoother is more computationally burdensome because it requires inverting \\(\\mat{P}_{t + 1}\\). The other state smoother only requires inverting \\(\\mat{F}_t\\), but that has already been inverted in the filtering pass. &lt;– ## Jacknife and Deleted observations Results in West and Harrison (1997), p. 104 and such. –&gt; References "],
["simulation-smoothers.html", "3.3 Simulation smoothers", " 3.3 Simulation smoothers 3.3.1 Mean corrections (Durbin and Koopman 2012, Sec 4.9) Simulation smoother by mean corrections. Ch. 4.9. TODO 3.3.2 de Jong-Shephard method While the mean-corrections method usually works, it may fail in some cases due to imposed ill-defined variance matrices (see Jungbacker and Koopman 2007, sec 1). These recursions were developed in de Jong and Shephard (1995), TODO 3.3.3 Forward-Filter Backwards smoother TODO References "],
["missing-observations.html", "3.4 Missing observations", " 3.4 Missing observations (Durbin and Koopman 2012, Sec 4.10) When all observations at time \\(t\\) are missing, the filtering recursions become, \\[ \\begin{aligned}[t] \\vec{a}_{t|t} &amp;= \\vec{a}_t , \\\\ \\mat{P}_{t|t} &amp;= \\mat{P}_t , \\\\ \\vec{a}_{t + 1} &amp;= \\mat{T}_t \\vec{a}_t + \\vec{c}_t \\\\ \\mat{P}_{t + 1} &amp;= \\mat{T}_t \\mat{P}_t \\mat{T}_t\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T \\end{aligned} \\] This is equivalent to setting \\(\\mat{Z}_t = \\mat{0}\\) (implying also that \\(\\mat{K}_t = \\mat{0}\\)) in the filtering equations. For smoothing, also replace \\(\\mat{Z}_t = \\mat{0}\\), \\[ \\begin{aligned}[t] \\vec{r}_{t - 1} &amp;= \\mat{T}_t\\T \\vec{r}_t , \\\\ \\mat{N}_{t - 1} &amp;= \\mat{T}_t\\T \\mat{N}_t \\mat{T}_t, \\end{aligned} \\] When some, but not all observations are missing, then replace the observation equation by, \\[ \\begin{aligned}[t] \\vec{y}^*_t &amp;= \\mat{Z}^*_t \\vec{\\alpha}_t + \\vec{\\varepsilon}_t^*, &amp; \\vec{\\varepsilon}_t^* &amp;\\sim N(\\vec{0}, \\mat{H}_t^*), \\end{aligned} \\] where, \\[ \\begin{aligned}[t] \\vec{y}^*_t &amp;= \\mat{W}_t \\vec{y}_t, \\\\ \\mat{Z}^* &amp;= \\mat{W}_t \\mat{Z}_t , \\\\ \\vec{\\varepsilon}_t &amp;= \\mat{W}_t \\vec{\\varepsilon}_t , \\\\ \\mat{H}^*_t &amp;= \\mat{W}_t \\mat{H}_t \\mat{W}_t\\T , \\end{aligned} \\] and \\(\\mat{W}_t\\) is a selection matrix to select non-missing values. In smoothing the missing elements are estimated by the appropriate elements of \\(\\mat{Z}_t \\hat{\\vec{alpha}}_t\\), where \\(\\hat{\\vec{\\alpha}}_t\\) is the smoothed state. References "],
["forecasting-matrices.html", "3.5 Forecasting matrices", " 3.5 Forecasting matrices (Durbin and Koopman 2012, Sec 4.11) Forecasting future observations are the same as treating the future observations as missing, \\[ \\begin{aligned}[t] \\bar{\\vec{y}}_{n + j} &amp;= \\mat{Z}_{n + j} \\bar{\\vec{a}}_{n + j} \\\\ \\bar{\\mat{F}}_{n + j} &amp;= \\mat{Z}_{n + j} \\bar{\\mat{P}}_{n + j} \\mat{Z}_{n + j}\\T + \\mat{H}_{n + j} . \\end{aligned} \\] --> References "],
["filtering-and-smoothing-1.html", "Chapter 4 Filtering and Smoothing ", " Chapter 4 Filtering and Smoothing "],
["filtering-1.html", "4.1 Filtering", " 4.1 Filtering From (Durbin and Koopman 2012, Sec 4.3) \\[ \\begin{aligned}[t] \\vec{v}_t &amp;= \\vec{y}_t - \\mat{Z}_t \\vec{a}_t - \\vec{d}_t, \\\\ \\mat{F}_t &amp;= \\mat{Z}_t \\mat{P}_t \\mat{Z}_t\\T + \\mat{H}_t, \\\\ \\vec{a}_{t|t} &amp;= \\vec{a}_t + \\mat{P}_t \\mat{Z}_t\\T \\mat{F}_t^{-1} v_t , \\\\ \\mat{P}_{t|t} &amp;= \\mat{P}_t - \\mat{P}_t \\mat{Z}_t\\T \\mat{F}_t^{-1} \\mat{Z}_t \\mat{P}_t , \\\\ \\vec{a}_{t + 1} &amp;= \\mat{T}_t \\vec{a}_t + \\mat{K}_t \\vec{v}_t + \\vec{c}_t \\\\ &amp;= \\mat{T}_{t} \\vec{a}_{t|t} + \\vec{c}_t, \\\\ \\mat{P}_{t + 1} &amp;= \\mat{T}_t \\mat{P}_t (\\mat{T}_t - \\mat{K}_t \\mat{Z}_t)\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T \\\\ &amp; = \\mat{T}_t \\mat{P}_{t|t} \\mat{T}_t\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T \\end{aligned} \\] The vector \\(\\vec{v}_t\\) are the *one-step ahead forecast errors$. The matrix \\(\\mat{K}_t\\) is called the Kalman gain, \\[ \\mat{K}_t = \\mat{T}_t \\mat{P}_t \\mat{Z}_t\\T \\mat{F}_t^{-1} . \\] This gives, \\[ \\begin{aligned}[t] \\vec{a}_{t + 1} &amp;= \\mat{T} \\vec{a}_{t|t} = \\mat{T}_t \\vec{a}_t + \\mat{K}_t \\vec{v}_t , \\\\ \\vec{P}_{t + 1} &amp;= \\mat{T}_t \\mat{P}_t \\left( \\mat{T}_t - \\mat{K}_t \\mat{Z}_t \\right)\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T . \\end{aligned} \\] These two equations are called the prediction step. The steps for \\(\\vec{a}_{t|t}\\) and \\(\\mat{P}_{t|t}\\) are called the updating step. \\[ \\begin{aligned}[t] \\vec{a}_t &amp;= \\E(\\vec{\\alpha}_t | y_{1, \\dots, t - 1}), &amp; \\vec{P}_t &amp;= \\Var(\\vec{\\alpha}_t | y_{1, \\dots, t - 1}), \\\\ \\vec{a}_{t|t} &amp;= \\E(\\vec{\\alpha}_t | y_{1, \\dots, t}), &amp; \\vec{P}_{t|t} &amp;= \\Var(\\vec{\\alpha}_t | y_{1, \\dots, t}) . \\end{aligned} \\] Dimensions of matrices and vectors in the SSM matrix/vector dimension \\(\\vec{v}_t\\) \\(p \\times 1\\) \\(\\vec{a}_t\\) \\(m \\times 1\\) \\(\\vec{a}_{t|t}\\) \\(m \\times 1\\) \\(\\mat{F}_t\\) \\(p \\times p\\) \\(\\mat{K}_t\\) \\(m \\times p\\) \\(\\mat{P}_t\\) \\(m \\times m\\) \\(\\mat{P}_{t|T}\\) \\(m \\times m\\) \\(\\vec{x}_t\\) \\(m \\times 1\\) \\(\\mat{L}_t\\) \\(m \\times m\\) See (Durbin and Koopman 2012, Sec 4.3.4): For a time-invariant state space model, the Kalman recursion for \\(\\mat{P}_{t + 1}\\) converges to a constant matrix \\(\\bar{\\mat{P}}\\), \\[ \\bar{\\mat{P}} = \\mat{T} \\bar{\\mat{P}} \\mat{T}\\T - \\mat{T} \\bar{\\mat{P}} \\mat{Z}\\T \\bar{\\mat{F}}^{-1} \\mat{Z} \\bar{\\mat{P}} \\mat{T}\\T + \\mat{R} \\mat{Q} \\mat{R}\\T , \\] where \\(\\bar{\\mat{F}} = \\mat{Z} \\bar{\\mat{P}} \\mat{Z}\\T + \\mat{H}\\). See (Durbin and Koopman 2012, Sec 4.3.5): The state estimation error is, \\[ \\vec{x}_t = \\vec{\\alpha}_t - \\vec{a}_t, \\] where \\(\\Var(\\vec{x}_t) = \\mat{P}_t\\). The \\(v_t\\) are sometimes called innovations, since they are the part of \\(\\vec{y}_t\\) not predicted from the past. The innovation analog of the state space model is \\[ \\begin{aligned}[t] \\vec{v}_t &amp;= \\mat{Z}_t \\vec{x}_t + \\vec{\\varepsilon}_t , \\\\ \\vec{x}_{t + 1} &amp;= \\mat{L} \\vec{x}_{t} + \\mat{R}_t \\vec{\\eta}_t - \\mat{K}_t \\vec{\\varepsilon}_t , \\\\ \\mat{K}_t &amp;= \\mat{T}_t \\mat{P}_t \\mat{Z}_t\\T \\mat{F}_t^{-1} , \\\\ \\mat{L}_t &amp;= \\mat{T}_t - \\mat{K}_t \\mat{Z}_t , \\mat{P}_{t + 1} &amp;= \\mat{T}_t \\mat{P}_t \\mat{L}_t\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_T\\T . \\end{aligned} \\] These recursions allow for a simpler derivation of \\(\\mat{P}_{t + 1}\\), and are useful for the smoothing recursions. Moreover, the one-step ahead forecast errors are indendendent, which allows for a simple derivation of the log-likelihood. Alternative methods TODO square-root filtering precision filters sequential filtering References "],
["smoothing-1.html", "4.2 Smoothing", " 4.2 Smoothing Dimensions of vectors and matrices used in smoothing recursions Vector/Matrix Dimension \\(\\vec{r}_t\\) \\(m \\times 1\\) \\(\\vec{\\vec{\\alpha}}_t\\) \\(m \\times 1\\) \\(\\vec{u}_t\\) \\(p \\times 1\\) \\(\\hat{\\vec{\\varepsilon}}_t\\) \\(p \\times 1\\) \\(\\hat{\\vec{\\eta}}_t\\) \\(r \\times 1\\) \\(\\mat{N}_t\\) \\(m \\times m\\) \\(\\mat{V}_t\\) \\(m \\times m\\) \\(\\mat{D}_t\\) \\(p \\times p\\) 4.2.1 State Smoothing (Durbin and Koopman 2012, Sec 4.4.2) Smoothing calculates the estimate of the state using all observations, \\(\\hat{\\vec{\\alpha}} = \\E(\\vec{\\alpha}_t | \\mat{Y}_n)\\) and \\(\\mat{V}_t = \\Var(\\vec{\\alpha} | \\mat{Y}_n)\\). Smoother in Eq 4.44. Let \\(\\hat{\\vec{a}} = \\E(\\alpha_t | \\vec{y}_1, \\dots \\vec{y}_n)\\) and \\(\\mat{V}_t = \\Var(\\vec{\\alpha}_t | \\vec{y}_1, \\dots \\vec{y}_n)\\), then \\[ \\begin{aligned}[t] \\vec{r}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{F}_t^{-1} \\vec{v}_t + \\mat{L}_t\\T \\vec{r}_t , \\\\ \\mat{N}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{F}_t^{-1} \\mat{Z}_t + \\mat{L}_t\\T \\mat{N}_t \\mat{L}_t, \\\\ \\hat{\\vec{\\alpha}}_t &amp;= \\vec{a}_t + \\mat{P}_t \\vec{r}_{t - 1} , \\\\ \\mat{V}_t &amp;= \\mat{P}_t - \\mat{P}_t \\mat{N}_{t - 1} \\mat{P}_t , \\end{aligned} \\] for \\(t = n, \\dots, 1\\), with \\(\\vec{r}_n = \\vec{0}\\), and \\(\\mat{N}_t = \\mat{0}\\). During the filtering pass \\(\\vec{v}_t\\), \\(\\mat{F}_t\\), \\(\\mat{K}_t\\), and \\(\\mat{P}_t\\) for \\(t = 1, \\dots, n\\) need to be stored. Alternatively, \\(\\vec{a}_t\\) and \\(\\mat{P}_t\\) only can be stored, and \\(\\vec{v}_t\\), \\(\\mat{F}_t\\), \\(\\mat{K}_t\\) recalculated on the fly. However, since the dimensions of \\(\\vec{v}_t\\), \\(\\mat{F}_t\\), \\(\\mat{K}_t\\) are usually small relative to \\(\\vec{a}_t\\) and \\(\\mat{P}_t\\) is is usually worth storing them. 4.2.2 Updating Smoothed States (Durbin and Koopman 2012, Sec 4.4.5) If a new observation arises, existing smoothed estimates of the states can be updated without having to run the full state smoother again. Suppose a new obervation, \\(\\vec{y}_{n + 1}\\), is available. We want to calculate \\(\\hat{\\vec{\\alpha}}_{t|n + 1} = \\E(\\vec{\\alpha} | \\mat{Y}_{n + 1})\\) and \\(\\mat{V}_{t|n + 1} = \\Var(\\vec{\\alpha}) | \\mat{Y}_{n + 1}\\), when we already have \\(\\hat{\\vec{\\alpha}}_{t|n} = \\E(\\vec{\\alpha} | \\mat{Y}_{n})\\) and \\(\\mat{V}_{t|n} = \\Var(\\vec{\\alpha} | \\mat{Y}_{n})\\) from running smoother when \\(\\mat{Y}_n\\) was available. Let \\(b_{t|n + 1} = \\mat{L}_t\\T \\cdots \\mat{L}_n\\T\\) with \\(\\vec{b}_{t|n + 1} = \\mat{I}_m\\). Then \\(b_{t|n + 1} = \\mat{L}_t\\T b_{t+1|n+1}\\) for \\(t = n, \\dots, 1\\). Then the states can be updated as, \\[ \\begin{aligned}[t] \\hat{\\vec{\\alpha}}_{t|n + 1} &amp;= \\hat{\\vec{a}}_{t|n} + \\mat{P}_t \\vec{b}_{t|n + 1} \\mat{Z}_{n + 1}\\T \\mat{F}_{n + 1}^{-1} \\vec{v}_{n + 1} , \\\\ \\mat{V}_{t|n + 1} &amp;= \\mat{V}_{t|n} - \\mat{P}_t \\vec{b}_{t|n + 1} \\mat{Z}_{n + 1}\\T \\mat{F}_{n + 1}^{-1} \\mat{Z}_{n + 1} \\vec{b}_{t|n + 1}\\T \\mat{P}_t , \\end{aligned} \\] for \\(n = t, t + 1, \\dots\\), with \\[ \\begin{aligned}[t] \\hat{\\vec{\\alpha}}_{n|n} &amp;= \\vec{a}_n + \\mat{P}_n \\mat{Z}_n\\T \\mat{F}_n^{-1} \\vec{v}_n, \\\\ \\mat{V}_{n|n} &amp;= \\mat{P}_n - \\mat{P}_n \\mat{Z}_n\\T \\mat{F}_n^{-1} \\mat{Z}_n \\mat{P}_n , \\end{aligned} \\] The values of \\(\\mat{P}_t\\), \\(\\mat{L}_t\\), \\(\\mat{F}_{n + 1}\\), and \\(\\vec{v}_{n + 1}\\) are available from the Kalman filter. 4.2.3 Disturbance smoothing (Durbin and Koopman 2012, Sec 4.5) Disturbance smoothing calculates the smoothed estimates, \\(\\hat{\\vec{\\varepsilon}}_t = \\E(\\vec{\\varepsilon} | \\mat{Y}_n)\\) and \\(\\hat{\\vec{\\eta}} = \\E(\\vec{\\eta}_t | \\mat{Y}_n)\\). \\[ \\begin{aligned}[t] \\hat{\\vec{\\varepsilon}}_t &amp;= \\mat{H}_t (\\mat{F}^{-1} \\vec{v}_t - \\mat{K}_t\\T \\vec{r}_t) , &amp; \\Var(\\vec{\\varepsilon}_t | \\mat{Y}_n) &amp;= \\mat{H}_t - \\mat{H}_t (\\mat{F}_t^{-1} + \\mat{K}_t\\T \\mat{N}_t \\mat{K}_t) \\mat{H}_t , \\\\ \\hat{\\vec{\\eta}}_t &amp;= \\mat{Q}_t \\mat{R}_t\\T \\vec{r}_t , &amp; \\Var(\\vec{\\eta}_t | \\mat{Y}_n) &amp;= \\mat{Q}_t - \\mat{Q}_t \\mat{R}_t\\T \\mat{N}_t \\mat{R}_t \\mat{Q}_t , \\\\ \\vec{r}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{F}_t^{-1} \\vec{v}_t + \\mat{L}_t\\T \\vec{r}_t , &amp; \\mat{N}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{F}_t^{-1} \\mat{Z}_t + \\mat{L}_t\\T \\mat{N}_t \\mat{L}_t \\end{aligned} \\] Alternatively, these equations can be rewritten as, \\[ \\begin{aligned}[t] \\hat{\\vec{\\varepsilon}}_t &amp;= \\mat{H}_t \\vec{u}_t , &amp; \\Var(\\vec{\\varepsilon}_t | \\mat{Y}_n) &amp;= \\mat{H}_t - \\mat{H}_t \\mat{D}_t \\mat{H}_t , \\\\ \\hat{\\vec{\\eta}}_t &amp;= \\mat{Q}_t \\mat{R}_t\\T \\vec{r}_t , &amp; \\Var(\\vec{\\eta}_t | \\mat{Y}_n) &amp;= \\mat{Q}_t - \\mat{Q}_t \\mat{R}_t\\T \\mat{N}_t \\mat{R}_t \\mat{Q}_t , \\\\ \\vec{u}_t &amp;= \\mat{F}^{-1} \\vec{v}_t - \\mat{K}_t\\T \\vec{r}_t , &amp; \\mat{D}_t &amp;= \\mat{F}_t^{-1} + \\mat{K}_t\\T \\mat{N}_t \\mat{K}_t , \\\\ \\vec{r}_{t - 1} &amp;= \\mat{Z}_t\\T \\vec{u}_t + \\mat{T}_t\\T \\vec{r}_t , &amp; \\mat{N}_{t - 1} &amp;= \\mat{Z}_t\\T \\mat{D}_t \\mat{Z}_t + \\mat{T}_t\\T \\mat{N}_t \\mat{T}_t - \\mat{Z}_t\\T \\mat{K}_t\\T \\mat{N}_t \\mat{T}_t - \\mat{T}_t\\T \\mat{N}_t \\mat{K}_t \\mat{Z}_t . \\end{aligned} \\] This reformulation can be computationally useful since it relies on the system matrices \\(\\mat{Z}_t\\) and \\(\\mat{T}_t\\) which are often sparse. The smoothing error \\(\\vec{u}_t\\) and \\(\\vec{r}_t\\) are important in their own right. The vector \\(\\vec{r}_t\\) is the scaled smoothed estimator of \\(\\vec{\\eta}_t\\). The disturbance smoothing recursions require only \\(\\vec{v}_t\\), \\(\\mat{f}_t\\), and \\(\\mat{K}_t\\) from the Kalman filter. Unlike the state smoother, they do not require the vector \\(\\vec{a}_t\\) or matrix \\(\\mat{P}_t\\). 4.2.4 Fast state smoothing (Durbin and Koopman 2012, Sec 4.6.2) If the variances of the states do not need to be calculated, then a faster smoothing algorithm can be used (Koopman 1993). The fast state smoother is defiend as, \\[ \\hat{\\vec{\\alpha}}_t = \\mat{T}_t \\hat{\\vec{\\alpha}}_t + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T \\vec{r}_t , \\] for \\(t = 1, \\dots, n\\). This is initialized via, \\[ \\hat{\\vec{\\alpha}}_1 = \\vec{a}_1 + \\mat{P}_1 \\vec{r}_0 . \\] Thus the smoother is run, but only calculating \\(\\vec{r}_n, \\dots, \\vec{r}_0\\). Then, \\(\\hat{\\vec{\\alpha}}_1, \\dots \\hat{\\vec{\\alpha}}_t\\) are calculated using the above equation. 4.2.5 Classical state smoothing See (Durbin and Koopman 2012, Sec 4.6.1) The original state smoothing algorithm from Anderson and Moore (1979) is \\[ \\begin{aligned}[t] \\hat{\\vec{\\alpha}}_t &amp;= \\vec{a}_{t|t} + \\mat{P}_{t|t} \\mat{T}_t\\T \\mat{P}_{t + 1}^{-1}(\\hat{\\vec{\\alpha}}_{t + 1} - \\vec{a}_{t + 1}) . \\end{aligned} \\] Note that \\(\\mat{T} \\mat{P}_{t|t} = \\mat{L}_t \\mat{P}_t\\). Relative to the other state smoothing algorithm the classical smoother is more computationally burdensome because it requires inverting \\(\\mat{P}_{t + 1}\\). The other state smoother only requires inverting \\(\\mat{F}_t\\), but that has already been inverted in the filtering pass. &lt;– ## Jacknife and Deleted observations Results in West and Harrison (1997), p. 104 and such. –&gt; References "],
["simulation-smoothers-1.html", "4.3 Simulation smoothers", " 4.3 Simulation smoothers 4.3.1 Mean corrections (Durbin and Koopman 2012, Sec 4.9) Simulation smoother by mean corrections. Ch. 4.9. TODO 4.3.2 de Jong-Shephard method While the mean-corrections method usually works, it may fail in some cases due to imposed ill-defined variance matrices (see Jungbacker and Koopman 2007, sec 1). These recursions were developed in de Jong and Shephard (1995), TODO 4.3.3 Forward-Filter Backwards smoother TODO References "],
["missing-observations-1.html", "4.4 Missing observations", " 4.4 Missing observations (Durbin and Koopman 2012, Sec 4.10) When all observations at time \\(t\\) are missing, the filtering recursions become, \\[ \\begin{aligned}[t] \\vec{a}_{t|t} &amp;= \\vec{a}_t , \\\\ \\mat{P}_{t|t} &amp;= \\mat{P}_t , \\\\ \\vec{a}_{t + 1} &amp;= \\mat{T}_t \\vec{a}_t + \\vec{c}_t \\\\ \\mat{P}_{t + 1} &amp;= \\mat{T}_t \\mat{P}_t \\mat{T}_t\\T + \\mat{R}_t \\mat{Q}_t \\mat{R}_t\\T \\end{aligned} \\] This is equivalent to setting \\(\\mat{Z}_t = \\mat{0}\\) (implying also that \\(\\mat{K}_t = \\mat{0}\\)) in the filtering equations. For smoothing, also replace \\(\\mat{Z}_t = \\mat{0}\\), \\[ \\begin{aligned}[t] \\vec{r}_{t - 1} &amp;= \\mat{T}_t\\T \\vec{r}_t , \\\\ \\mat{N}_{t - 1} &amp;= \\mat{T}_t\\T \\mat{N}_t \\mat{T}_t, \\end{aligned} \\] When some, but not all observations are missing, then replace the observation equation by, \\[ \\begin{aligned}[t] \\vec{y}^*_t &amp;= \\mat{Z}^*_t \\vec{\\alpha}_t + \\vec{\\varepsilon}_t^*, &amp; \\vec{\\varepsilon}_t^* &amp;\\sim N(\\vec{0}, \\mat{H}_t^*), \\end{aligned} \\] where, \\[ \\begin{aligned}[t] \\vec{y}^*_t &amp;= \\mat{W}_t \\vec{y}_t, \\\\ \\mat{Z}^* &amp;= \\mat{W}_t \\mat{Z}_t , \\\\ \\vec{\\varepsilon}_t &amp;= \\mat{W}_t \\vec{\\varepsilon}_t , \\\\ \\mat{H}^*_t &amp;= \\mat{W}_t \\mat{H}_t \\mat{W}_t\\T , \\end{aligned} \\] and \\(\\mat{W}_t\\) is a selection matrix to select non-missing values. In smoothing the missing elements are estimated by the appropriate elements of \\(\\mat{Z}_t \\hat{\\vec{alpha}}_t\\), where \\(\\hat{\\vec{\\alpha}}_t\\) is the smoothed state. References "],
["forecasting-matrices-1.html", "4.5 Forecasting matrices", " 4.5 Forecasting matrices (Durbin and Koopman 2012, Sec 4.11) Forecasting future observations are the same as treating the future observations as missing, \\[ \\begin{aligned}[t] \\bar{\\vec{y}}_{n + j} &amp;= \\mat{Z}_{n + j} \\bar{\\vec{a}}_{n + j} \\\\ \\bar{\\mat{F}}_{n + j} &amp;= \\mat{Z}_{n + j} \\bar{\\mat{P}}_{n + j} \\mat{Z}_{n + j}\\T + \\mat{H}_{n + j} . \\end{aligned} \\] --> References "],
["parameter-estimation.html", "Chapter 5 Parameter Estimation ", " Chapter 5 Parameter Estimation "],
["log-log-likelihood.html", "5.1 Log log-likelihood", " 5.1 Log log-likelihood If the the system matrices and initial conditions are known, the log likelihood is \\[ \\begin{aligned}[t] \\log L(\\mat{Y}_n) &amp;= \\log p(\\vec{y}_1, \\dots, \\vec{y}_n) = \\sum_{t = 1}^n \\log p(\\vec{y}_t | \\mat{Y}_{t - 1}) \\\\ &amp;= - \\frac{np}{2} \\log 2 \\pi - \\frac{1}{2} \\sum_{t = 1}^n \\left( \\log \\left| \\mat{F}_t \\right| + \\vec{v}\\T \\mat{F}_t^{-1} \\vec{v}_t \\right) \\end{aligned} . \\] The log-likelihood only requires running the filter to calculate \\(\\mat{F}_t\\) and \\(\\mat{V}_t\\). See (Durbin and Koopman 2012, Sec. 7.2.1) References "],
["integrated-sampler.html", "5.2 Integrated Sampler", " 5.2 Integrated Sampler \\[ p(\\vec{y} | \\mat{H}, \\mat{\\Psi}) = \\int p(\\vec{y} | \\vec{\\alpha}, \\mat{H}) p(\\vec{\\alpha} | \\mat{\\Psi})\\,d\\vec{\\alpha}, \\] The log-likelihood of a state-space function can be calculated analytically and expressed up to an integrating constant marginally of the state vector \\(\\vec{\\alpha}\\), \\[ \\log p(\\vec{y} | \\mat{H}, \\mat{\\Psi}) = \\text{const} - 0.5 \\left( \\sum_{t = 1}^n \\sum_{i = 1}^p \\log f_{t, i} - v_t^2 f^{-1}_{t,i} \\right) . \\] Thus, parameters of the state space model can be sampled as, \\[ p(\\mat{H}, \\mat{\\Psi} | \\vec{y}) \\propto p(\\vec{y} | \\mat{H}, \\mat{\\Psi}) p(\\mat{H}) p(\\mat{\\Psi}) . \\] "],
["diagnostic-checking.html", "5.3 Diagnostic Checking", " 5.3 Diagnostic Checking The standardized prediction errors are, \\[ \\vec{v}^*_t = \\mat{G}_t \\vec{v}_t , \\] where \\(\\mat{G}_t \\mat{G}_t\\T = \\mat{F}_t^{-1}\\). See Koopmans JSS Sec 3.3. These residuals should satisfy independence, homoskedasticity, and normality. independence: Box-Ljung test statistic normality: Bowman and Shenton test statistic homoskedasticity: compare variance of standardized prediction errors of the 1st third to that of the last third. See Harvey (1989), Durbin and Koopman (2012), and Commandeur and Koopman (2007). The auxiliary residuals are the standardized smoothed observation and state disturbances, \\[ \\begin{aligned}[t] e^*_t &amp;= \\frac{\\hat{\\varepsilon}_t}{\\sqrt{\\Var(\\hat{varepsilon}_t)}} , \\\\ r^*_t &amp;= \\frac{\\hat{\\eta}_t}{\\sqrt{\\Var{\\hat{\\eta}_t}}} , \\end{aligned} \\] for \\(t = 1, \\dots, n\\). The standardized smoothed observation disturbances allows for the detection of outliers, while the standardized smoothed state disturbances allows for the detection of structural breaks. Each auxiliary residual is a \\(t\\)-test that there was no outlier (or structural break). -->"],
["example-models.html", "Chapter 6 Example Models", " Chapter 6 Example Models 6.0.1 Polynomial Trend Models See (G. Petris, Petrone, and Campagnoli 2009, Sec 3.2; West and Harrison 1997, Ch 7) A polynomial trend model of order \\(n\\) has constant matrices \\(\\mat{T}_t = \\mat{T}\\), \\(\\mat{Z}_t = \\mat{Z}\\), and has a forecast function of the form, \\[ f_t(k) = \\E(y_{t + k} | \\vec{Y}_t) = a_{t,0} + a_{t,1} k + \\cdots + a_{t, n - 1} k^{n -1}, \\quad k \\geq 0, \\] 6.0.1.1 Local Level Model A special case of polynomial trend models is the local level model or random walk plus noise. It is defined as, \\[ \\begin{aligned}[t] y_t &amp;= \\alpha_{t} + \\varepsilon_t &amp; \\varepsilon_t &amp;\\sim N(0, \\sigma^{\\varepsilon}^2)\\\\ \\alpha_{t + 1} &amp;= \\alpha_{t} + \\eta_t &amp; \\eta_t &amp;\\sim N(0, \\sigma_{\\eta}^2) \\end{aligned} \\] This is a SSM with, \\[ \\mat{T} = \\mat{Z} = \\mat{R} = \\begin{bmatrix} 1 \\end{bmatrix} , \\] and \\(Q = \\sigma_{\\eta}^2\\) and \\(H = \\sigma_{\\varepsilon}^2\\). This model is equivalent to the intercept of a classical linear regression model, \\[ y_t = \\mu + \\varepsilon_t , \\] but in which the the intercept \\(\\mu\\) is allowed to vary with time. See (Commandeur and Koopman 2007, Ch 2; West and Harrison 1997) for more details. 6.0.1.1.1 Linear Growth Model The state vector of a linear growth models is \\(\\vec{\\alpha}_t = (\\mu_t, \\beta_t)&#39;\\), where \\(\\mu_t\\) is interpreted as the local level, and \\(\\beta_t\\) is interpreted as the local growth rate. \\[ \\begin{aligned}[t] y_t &amp;= \\mu_t + \\varepsilon_t &amp; \\varepsilon_t &amp;\\sim N(0, H) \\\\ \\mu_{t + 1} &amp;= \\mu_{t} + \\beta_{t} + \\eta_{\\mu,t} \\\\ \\beta_{t + 1} &amp;= \\beta_{t} + \\eta_{\\beta,t} &amp; \\vec{\\eta}_t = (\\eta_{\\mu,t}, \\eta_{\\beta,t})\\T &amp;\\sim N(0, \\mat{Q}) \\end{aligned} \\] which can be written as, \\[ \\begin{aligned}[t] y_t &amp;= \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\vec{\\alpha}_t + \\varepsilon_t \\\\ \\vec{\\alpha}_{t + 1} &amp;= \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} \\vec{\\alpha}_t + \\vec{\\eta}_t &amp; \\vec{\\eta}_t &amp;\\sim N(0, \\mat{Q}) \\end{aligned} \\] This is a SSM with, \\[ \\begin{aligned}[t] \\mat{Z} &amp;= \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\\\ \\mat{T} &amp;= \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} \\\\ \\mat{R} &amp;= \\mat{I}_2 \\end{aligned} \\] 6.0.1.2 nth-Order Polynomial Models Then, \\[ \\begin{aligned}[t] y_t &amp;= \\mu_t + \\varepsilon_t \\\\ \\alpha_{1,t + 1} &amp;= \\alpha_{1,t} + \\alpha_{2,t} + \\cdots + \\alpha_{n,t} + \\eta_{1,t} \\\\ \\alpha_{2,t + 1} &amp;= \\alpha_{2,t} + \\cdots + \\alpha_{n,t} + \\eta_{2,t} \\\\ \\vdots &amp;= \\vdots \\\\ \\alpha_{n,t + 1} &amp;= \\alpha_{n,t} + \\eta_{n,t} \\\\ \\end{aligned} \\] which can be written as, \\[ \\begin{aligned}[t] y_t &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{bmatrix} \\vec{\\alpha}_{t} + \\varepsilon_t \\\\ \\vec{\\alpha}_{t + 1} &amp;= \\mat{U}_n \\vec{\\alpha}_{t} + \\mat{I}_n \\vec{\\eta}_t \\end{aligned} \\] where \\(\\mat{U}_n\\) is an \\(n \\times n\\) upper triangular matrix of unit elements, \\[ \\mat{U}_n = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\end{bmatrix} \\] This is a SSM with, \\[ \\begin{aligned}[t] \\mat{Z} &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\end{bmatrix} \\\\ \\mat{T} &amp;= \\mat{U}_n \\\\ \\mat{R} &amp;= \\mat{I}_n \\end{aligned} \\] In general, \\(\\mat{Q}\\) can be unstructured. Since the scale of the derivatives are likely related, it makes little sense to allow \\(\\mat{Q}\\) to be diagonal. (West and Harrison 1997, Sec 7.1) suggest using the following structured form of \\(\\mat{Q}\\) in order to represent the correlations between states, \\[ \\begin{aligned}[t] \\mat{R} &amp;= \\mat{U}_n \\\\ \\mat{Q} &amp;= \\diag(q_1, q_2, \\dots, q_n) \\end{aligned} \\] Since \\(\\mat{Q}\\) is diagonal, the errors \\(\\eta\\) are uncorrelated, but \\(\\mat{R} \\mat{Q} \\mat{R}\\T\\) allows for correlations between the states. 6.0.2 Seasonal Factor Models (G. Petris, Petrone, and Campagnoli 2009, Sec 3.2.2) A seasonal factor model with \\(s\\) seasons is, \\[ \\begin{aligned}[t] y_t &amp;= \\alpha_{1,t} + \\varepsilon_t \\\\ \\alpha_{1,t + 1} &amp;= -\\alpha_{1,t} -\\alpha_{2,t} - \\cdots -\\alpha_{s,t} + \\eta_{1,t} \\\\ \\alpha_{2,t + 1} &amp;= \\alpha_{1,t} \\\\ \\alpha_{3,t + 1} &amp;= \\alpha_{2,t} \\\\ \\vdots &amp;= \\vdots \\\\ \\alpha_{s,t + 1} &amp;= \\alpha_{s - 1,t} \\end{aligned} \\] Note that all of the states evolve deterministically except for \\(\\alpha_1\\). This can be written in SSM form as, \\[ \\begin{aligned}[t] y_t &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\end{bmatrix} \\vec{\\alpha}_t + \\varepsilon_t &amp; \\varepsilon_t &amp; \\sim N(0, H) \\\\ \\vec{\\alpha}_{t + 1} &amp;= \\begin{bmatrix} -1 &amp; -1 &amp; \\dots &amp; -1 &amp; -1 \\\\ 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0 \\end{bmatrix} \\vec{\\alpha}_{t} + \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\eta_t &amp; \\eta_t &amp;\\sim N(0, Q) \\end{aligned} \\] This is a case of a SSM in which \\(r = 1 &lt; m = s\\). Also, note that only the first state has a non-zero system disturbance. This could be written with alternative forms of \\(\\mat{R}\\) and \\(\\mat{Q}\\), e.g. \\(\\mat{R} = \\mat{I}_m\\) and a diagonal \\(\\mat{Q}\\), but since none of the other states are tied to the observation model, it is equivalent. 6.0.3 Fourier Form Seasonal Models [G. Petris, Petrone, and Campagnoli (2009), Ch 3.2.3-3.2.4; ] 6.0.4 ARMA and ARIMA Models In ARMA modeling, trend and season components are differenced and the resulting differenced series is modeled as a stationary time series. Let \\(\\Delta y_t = y_t - y_{t - 1}\\), \\(\\Delta^2 = \\Delta(\\Delta y_t)\\), \\(\\Delta_s y_t = y_t - y_{t - s}\\), \\(\\Delta^2_s y_t = \\Delta_s (\\Delta_s y_t)\\), and so on. Difference into trend and season effects have been eliminated, giving the variable, \\(y^*_t = \\Delta^d \\Delta_s^D y_t\\) for \\(d, D = 0, 1, \\dots\\), which is modeled as a an ARMA(\\(p\\), \\(q\\)) process, \\[ \\begin{aligned}[t] y^*_t &amp;= \\phi_1 y^*_{t - 1} + \\cdots + \\phi_p y^*_{t - p} + \\zeta_t + \\theta_1 \\zeta_{t - 1} + \\cdots + \\theta_q \\zeta_{t - q}, &amp; \\zeta_t &amp;\\sim N(0, \\sigma_{\\zeta}^2) \\\\ &amp;= \\sum_{j = 1}^{p} \\phi_j y^*_{t - j} + \\zeta_t + \\sum_{j = 1}^{q} \\theta_j \\zeta_{t - j} \\\\ &amp;= \\sum_{j = 1}^{r} \\phi_j y^*_{t - j} + \\zeta_t + \\sum_{j = 1}^{r - 1} \\theta_j \\zeta_{t - j} . \\end{aligned} \\] In the third equation, \\(r = \\max(p, q + 1)\\), and \\(\\phi_j = 0\\) if \\(j &gt; p\\), and \\(\\theta_j\\) equation, some coefficients can be zero. A state-space form of an ARMA model uses the following latent states, \\[ \\vec{\\alpha}_t = \\begin{pmatrix} y_t \\\\ \\phi_2 y_{t - 1} + \\cdots + \\phi_r y_{t - r + 1} + \\theta_1 \\zeta_t + \\cdots + \\theta_{r - 1} \\zeta_{t - r + 1} \\\\ \\phi_3 y_{t - 1} + \\cdots + \\phi_r y_{t - r + 2} + \\theta_2 \\zeta_t + \\cdots + \\theta_{r - 1} \\zeta_{t - r + 3} \\\\ \\vdots \\\\ \\phi_r y_{t - 1} + \\theta_{r - 1} \\zeta_{t} \\\\ \\end{pmatrix} \\] The system matrices are, \\[ \\begin{aligned}[t] \\mat{Z}_t = \\mat{Z} = &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{bmatrix} \\\\ \\mat{H}_t = \\mat{H} = &amp;= 0 \\\\ \\mat{T}_t = \\mat{T} = &amp;= \\begin{bmatrix} \\phi_1 &amp; 1 &amp; &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; \\\\ \\phi_{r - 1} &amp; 0 &amp; &amp; 1 \\\\ \\phi_{r} &amp; 0 &amp; \\cdots &amp; 0 \\end{bmatrix} \\\\ \\mat{R}_t = \\mat{R} &amp;= \\begin{bmatrix} 1 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_{r - 1} \\end{bmatrix} \\\\ \\eta &amp;= \\zeta_{t + 1} \\end{aligned} \\] Instead of differencing prior to analysis, the differencing can be done within the state-space model. For example, an ARIMA model with \\(p = 2\\), \\(d = 2\\), and \\(q = 1\\) is \\[ \\begin{aligned}[t] y_t &amp;= \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 0 \\end{bmatrix} \\vec{\\alpha}_t \\\\ \\alpha_{t + 1} &amp;= \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\phi_1 &amp; 1 \\\\ 0 &amp; 0 &amp; \\phi_2 &amp; 0 \\end{bmatrix} \\vec{\\alpha}_t + \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ \\theta_1 \\end{bmatrix} \\zeta_{t + 1} , \\end{aligned} \\] with \\[ \\vec{\\alpha}_t = \\begin{bmatrix} y_{t - 1} \\\\ \\Delta y_{t - 1} \\\\ y^*_t \\\\ \\phi_2 y^*_{t - 1} + \\theta_1 \\zeta_t \\end{bmatrix} \\] and \\(y^*_t = \\Delta^2 y_t = \\Delta (y_t - y_{t - 1})\\). The unknown non-stationary values of \\(y_0\\) and \\(\\Delta y_0\\) in the initial state vector \\(\\vec{\\alpha}_1\\) need to be initialized. This approach can easily extend to different levels of differencing and seasonal differencing. The \\(\\max(p, q + 1)\\) is not the only state space version of the ARMA model. TODO other representations. For examples of models see Eric Zivot’s notes 6.0.5 AR(2) The AR(2) model is, \\[ \\begin{aligned}[t] y_t &amp;= \\mu + \\phi_1 y_{t - 1} + \\phi_2 y_{t - 2} + \\eta_t &amp; \\eta_t \\sim N(0, \\sigma^2) . \\end{aligned} \\] One way to represent it is to define the states as \\[ \\vec{\\alpha}_t = (y_t, y_{t - 1})&#39;, \\] so that the state space form is, \\[ y_t = \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} y_t \\\\ y_{t - 1} \\\\ \\end{bmatrix} + \\eta_t \\\\ \\begin{bmatrix} y_t \\\\ y_{t - 1} \\end{bmatrix} &amp;= \\begin{bmatrix} \\mu \\\\ 0 \\end{bmatrix} \\begin{bmatrix} \\phi_1 &amp; \\phi_2 \\\\ 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} y_t \\\\ y_{t - 2} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\eta_t . \\] An alternate state space representation of AR(2) models is \\[ \\begin{aligned} y_t &amp;= \\mu + \\gamma_t \\\\ \\gamma_t &amp;= \\phi_1 \\gamma_{t - 1} + \\phi_2 \\gamma_{t - 2} + \\eta_t \\end{aligned} \\] The state vector is \\(\\vec{\\alpha}_t = (\\gamma_t, \\gamma_{t- 1})&#39;\\), and the observation and state quations are, \\[ \\begin{aligned}[t] y_t &amp;= \\mu + \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\gamma_t \\\\ \\gamma_{t - 1} \\end{bmatrix} \\\\ \\begin{bmatrix} \\gamma_t \\\\ \\gamma_{t - 1} \\end{bmatrix} &amp;= \\begin{bmatrix} \\phi_1 &amp; \\phi_2 \\\\ 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\gamma_t \\\\ \\gamma_{t - 1} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\eta_t \\end{aligned} \\] Another representation is, The state vector is \\(\\vec{\\alpha}_t = (y_t, \\phi_2 y_{t- 1})&#39;\\), and the observation and state quations are, \\[ \\begin{aligned}[t] y_t &amp;= \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\vec{\\alpha}_t \\\\ \\vec{\\alpha}_t &amp;= \\begin{bmatrix} \\phi_1 &amp; 1 \\\\ \\phi_2 &amp; 0 \\end{bmatrix} \\vec{\\alpha}_t + \\begin{bmatrix} \\mu \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\eta_t \\end{aligned} \\] 6.0.6 AR(p) Form in (West and Harrison 1997, 297) A SSM form of the AR(p) model, \\[ y_t = \\mu + \\sum_{j = 1}^p \\phi_j (y_{t - j} - \\mu) + \\varepsilon_t \\] where \\[ \\begin{aligned}[t] y_t &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\end{bmatrix} \\vec{\\alpha}_t \\\\ \\vec{\\alpha}_{t + 1} &amp;= \\begin{bmatrix} \\phi_1 &amp; \\phi_2 &amp; \\phi_3 &amp; \\cdots &amp; \\phi_p \\\\ 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0 \\end{bmatrix} \\vec{\\alpha}_t + \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\eta_t &amp; \\eta_t &amp;\\sim N(0, Q) \\end{aligned} \\] Note that \\(r = 1\\). References "],
["example-models-1.html", "Chapter 7 Example Models ", " Chapter 7 Example Models "],
["nile.html", "7.1 Nile", " 7.1 Nile This is a short (\\(n = 100\\)) univariate time series of the annual flow volumes of the Nile River at Aswan between 1871 and 1970. This series is described in Durbin and Koopman (2012) and had been analyzed by Cobb (1978) and Balke (1993). It is included with R as the dataset Nile. data(&quot;Nile&quot;, package = &quot;datasets&quot;) Nile_ &lt;- data_frame(year = year(ts_to_date(time(Nile))), flow = as.numeric(Nile)) ggplot(Nile_, aes(x = year, y = flow)) + geom_point() + geom_line() + ylab(&quot;Annual Flow&quot;) + xlab(&quot;&quot;) 7.1.1 Local Level Model The Nile data can be modeled as a local level model, \\[ \\begin{aligned}[t] y_t &amp;= \\mu_t + \\varepsilon_t &amp; \\varepsilon_t &amp; \\sim N(0, \\sigma_{\\varepsilon}^2) \\\\ \\mu_{t + 1} &amp;= \\mu_t + \\eta_t &amp; \\eta_t &amp; \\sim N(0, \\sigma^2_{\\eta}) \\end{aligned} \\] This is a time-invariant SSM with \\(m = 1\\) states, \\(p = 1\\) observations at each time period, and \\(r = 1\\) system disturbances, \\[ \\begin{aligned}[t] \\mat{T} = \\mat{Z} = \\mat{R} &amp;= \\begin{bmatrix} 1 \\end{bmatrix} \\\\ \\mat{c} = \\mat{Q} &amp;= \\begin{bmatrix} 0 \\end{bmatrix} \\\\ \\mat{H} &amp;= \\begin{bmatrix} \\sigma_{\\varepsilon}^2 \\end{bmatrix} \\\\ \\mat{Q} &amp;= \\begin{bmatrix} \\sigma_{\\eta}^2 \\end{bmatrix} \\end{aligned} \\] StructTS(Nile, type = &quot;level&quot;) ## ## Call: ## StructTS(x = Nile, type = &quot;level&quot;) ## ## Variances: ## level epsilon ## 1469 15099 References "],
["airline-passenger-miles.html", "7.2 Airline Passenger Miles", " 7.2 Airline Passenger Miles Monthly totals of international airline passengers, 1949 to 1960, used in the Box-Jenkins textbook (Box, Jenkins, and Reinsel 1994). This data is included with R as the dataset AirPassengers. data(&quot;AirPassengers&quot;, package = &quot;datasets&quot;) AirPassengers_ &lt;- tbl_df(as.data.frame(AirPassengers)) %&gt;% mutate(date = ts_to_date(time(AirPassengers))) %&gt;% select(date, everything()) %&gt;% mutate(x = as.numeric(x)) %&gt;% rename(passengers = x) %&gt;% mutate(log_passengers = log10(passengers)) ggplot(AirPassengers_, aes(x = date, y = passengers)) + geom_point() + geom_line() + scale_y_continuous(&quot;Number&quot;) + xlab(&quot;&quot;) See help for AirPassengers in R for example models. SARIMA (0, 1, 1) (0, 1, 1) as in Box-Jenkins arima(log10(AirPassengers), c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 12)) ## ## Call: ## arima(x = log10(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, ## 1, 1), period = 12)) ## ## Coefficients: ## ma1 sma1 ## -0.4018 -0.5569 ## s.e. 0.0896 0.0731 ## ## sigma^2 estimated as 0.0002543: log likelihood = 353.96, aic = -701.92 Structural Time Series model StructTS(log10(AirPassengers), type = &quot;BSM&quot;) ## ## Call: ## StructTS(x = log10(AirPassengers), type = &quot;BSM&quot;) ## ## Variances: ## level slope seas epsilon ## 0.0001456 0.0000000 0.0002635 0.0000000 References "],
["road-casualties-in-great-britain.html", "7.3 Road Casualties in Great Britain", " 7.3 Road Casualties in Great Britain Monthly totals of car drivers in Great Britain killed or seriously injured January 1969 to December 1984. The compulsory wearing of seat belts was introducted on January 31, 1983. These data were analyzed in A. C. Harvey and Durbin (1986), and used as an example in Durbin and Koopman (2012), Andrew C. Harvey (1993), and Commandeur and Koopman (2007). This data is included in R with the dataset Seatbelts. data(&quot;Seatbelts&quot;, package = &quot;datasets&quot;) Seatbelts_ &lt;- tbl_df(as.data.frame(Seatbelts)) %&gt;% mutate(date = ts_to_date(time(Seatbelts))) %&gt;% select(date, everything()) ggplot(Seatbelts_, aes(x = date, y = DriversKilled)) + geom_point() + geom_line() + ylab(&quot;Number&quot;) + xlab(&quot;&quot;) StructTS Models of Seatbelts StructTS(Seatbelts[ , &quot;DriversKilled&quot;], type = &quot;level&quot;) ## ## Call: ## StructTS(x = Seatbelts[, &quot;DriversKilled&quot;], type = &quot;level&quot;) ## ## Variances: ## level epsilon ## 342.42 68.78 StructTS(Seatbelts[ , &quot;DriversKilled&quot;], type = &quot;trend&quot;) ## ## Call: ## StructTS(x = Seatbelts[, &quot;DriversKilled&quot;], type = &quot;trend&quot;) ## ## Variances: ## level slope epsilon ## 346.58 0.00 66.32 StructTS(Seatbelts[ , &quot;DriversKilled&quot;], type = &quot;BSM&quot;) ## ## Call: ## StructTS(x = Seatbelts[, &quot;DriversKilled&quot;], type = &quot;BSM&quot;) ## ## Variances: ## level slope seas epsilon ## 43.83 0.00 39.85 124.33 SARIMA (1, 0, 0) (1, 0, 0) arima(Seatbelts[ , &quot;DriversKilled&quot;], c(1, 0, 0), seasonal = list(order = c(1, 0, 0))) ## ## Call: ## arima(x = Seatbelts[, &quot;DriversKilled&quot;], order = c(1, 0, 0), seasonal = list(order = c(1, ## 0, 0))) ## ## Coefficients: ## ar1 sar1 intercept ## 0.4919 0.4788 121.5958 ## s.e. 0.0686 0.0699 4.5267 ## ## sigma^2 estimated as 308.7: log likelihood = -824.44, aic = 1656.89 SARIMA with covariates X &lt;- Seatbelts[, c(&quot;kms&quot;, &quot;PetrolPrice&quot;, &quot;law&quot;)] X[, 1] &lt;- log10(X[, 1]) - 4 arima(log10(Seatbelts[, &quot;drivers&quot;]), c(1, 0, 0), seasonal = list(order = c(1, 0, 0)), xreg = X) ## ## Call: ## arima(x = log10(Seatbelts[, &quot;drivers&quot;]), order = c(1, 0, 0), seasonal = list(order = c(1, ## 0, 0)), xreg = X) ## ## Coefficients: ## ar1 sar1 intercept kms PetrolPrice law ## 0.3348 0.6672 3.3539 0.0082 -1.2224 -0.0963 ## s.e. 0.0775 0.0612 0.0441 0.0902 0.3839 0.0166 ## ## sigma^2 estimated as 0.001476: log likelihood = 349.73, aic = -685.46 References "],
["uk-quarterly-gas-consumption.html", "7.4 UK Quarterly Gas Consumption", " 7.4 UK Quarterly Gas Consumption Quarterly UK gas consumption from 1960 to 1986 in millions of therms. These data are used as an example in Durbin and Koopman (2012), and included in R as a dataset UKgas. UKgas_ &lt;- data_frame(date = ts_to_date(time(UKgas)), therms = as.numeric(UKgas)) ggplot(UKgas_, aes(x = date, y = therms)) + geom_point() + geom_line() + scale_y_log10(&quot;log10(therms)&quot;) + scale_x_date(date_breaks = &quot;5 years&quot;, date_labels = &quot;%Y&quot;) + xlab(&quot;&quot;) References "],
["other-sources.html", "7.5 Other Sources", " 7.5 Other Sources Giovanni Petris and Petrone (2011) - Nile: local level - UKGas: Structural: local trend + seasonal factor - Industrial Production of consumer goods: local trend, outliers and structural breaks. Giovanni Petris (2010) Temperuratures: dlm dynamic factor - Nile: local level Tusell (2011) - Commandeur and Koopman (2007) - Durbin and Koopman (2012) - KFAS - Shumway and Stoffer (2010) --> References "],
["stan-functions.html", "Chapter 8 Stan Functions ", " Chapter 8 Stan Functions "],
["misc.html", "8.1 Misc", " 8.1 Misc 8.1.1 make_symmetric_matrix int make_symmetric_matrix(matrix x) Ensure a matrix is symmetric. This returns \\(0.5 * (x + x&#39;)\\). 8.1.2 to_matrix_colwise matrix to_matrix_colwise(vector v, int r, int c) Return a \\(r \\times c\\) matrix from a vector \\(v\\) with \\(r \\times c\\). This function assumes that elements in the vector are column-major. 8.1.3 to_matrix_rowwise matrix to_matrix_rowwise(vector v, int r, int c) Return a \\(r \\times c\\) matrix from a length \\(r c\\) vector \\(v\\). This function assumes that elements in the vector are row-major. 8.1.4 to_vector_colwise vector to_vector_colwise(matrix x) Return a lengths \\(r c\\) vector from a length \\(r \\times c\\) matrix \\(x\\). This function fills in elements column-major. 8.1.5 to_vector_rowwise vector to_vector_rowwise(matrix x) Return a length \\(r c\\) vector from a \\(r \\times c\\) matrix \\(x\\). This function fills in elements row-major. "],
["filtering-2.html", "8.2 Filtering", " 8.2 Filtering matrix ssm_filter_predict_obs(vector y, matrix Z, matrix H, vector a, matrix P) Given \\(p \\times 1\\) vector \\(\\vec{a}_t = \\E(\\alpha_t | Y_{t - 1})\\) and \\(\\mat{P}_t = \\Var(\\alpha_t | Y_{t - 1})\\), calculate the one-step ahead forecast error \\(v_t = y_t - \\E(y_t | Y_{t - 1})\\) and the precision of the forecast, \\(F^{-1}_t = \\Var(y_t | Y_{t - 1})^{-1}\\). This returns a \\(p \\times (p + 1)\\) matrix containing both \\(v_t\\) and \\(F^{-1}\\), \\[ \\begin{bmatrix} v_t &amp; F^{-1}_t \\end{bmatrix} = \\begin{bmatrix} v_{t,1} &amp; F^{-1}_{t, 1, 1} &amp; F^{-1}_{t, 1, 2} &amp; \\cdots &amp; F^{-1}_{t, 1, p} \\\\ v_{t,2} &amp; F^{-1}_{t, 2, 1} &amp; F^{-1}_{t, 2, 2} &amp; \\cdots &amp; F^{-1}_{t, 2, p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ v_{t,p} &amp; F^{-1}_{t, p, 1} &amp; F^{-1}_{t, p, 2} &amp; \\cdots &amp; F^{-1}_{t, p, p} \\end{bmatrix} . \\] matrix ssm_filter_update(...) Given \\(p \\times 1\\) vector \\(\\vec{a}_t = \\E(\\alpha_t | Y_{t - 1})\\) and \\(\\mat{P}_t = \\Var(\\alpha_t | Y_{t - 1})\\), the one-step ahead forecast error \\(v_t = y_t - \\E(y_t | Y_{t - 1})\\), and the precision of the forecast, \\(F^{-1}_t = \\Var(y_t | Y_{t - 1})^{-1}\\), calculate the filtered state \\(a_{t| t} = \\E(\\alpha_t | Y_t)\\) and \\(P_{t | t} = \\Var(\\alpha_t | Y_t)\\). Returns a \\(m \\times (1 + m)\\) matrix with \\(a_{t|t}\\) and \\(P_{t|t}\\), \\[ \\begin{bmatrix} a_{t|t} &amp; P_{t|t} \\end{bmatrix} = \\begin{bmatrix} a_{t|t,1} &amp; P_{t|t, 1, 1} &amp; P_{t|t, 1, 2} &amp; \\cdots &amp; P_{t|t, 1, m} \\\\ a_{t|t,2} &amp; P_{t|t, 2, 1} &amp; P_{t|t, 2, 2} &amp; \\cdots &amp; P_{t|t, 2, m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{t|t,m} &amp; P_{t|t, m, 1} &amp; P_{t|t, m, 2} &amp; \\cdots &amp; P_{t|t, m, m} \\end{bmatrix} . \\] matrix ssm_filter_predict_state(...) Given \\(p \\times 1\\) vector \\(\\vec{a}_t = \\E(\\alpha_t | Y_{t - 1})\\) and \\(\\mat{P}_t = \\Var(\\alpha_t | Y_{t - 1})\\), the one-step ahead forecast error \\(v_t = y_t - \\E(y_t | Y_{t - 1})\\), and the precision of the forecast, \\(F^{-1}_t = \\Var(y_t | Y_{t - 1})^{-1}\\), predict the state in the next period, \\(a_{t+ t} = \\E(\\alpha_{t + 1}| Y_t)\\) and \\(P_{t + t1} = \\Var(\\alpha_{t + 1} | Y_t)\\). Returns a \\(p \\times (1 + p)\\) matrix with \\(a_{t + 1}\\) and \\(P_{t + 1}\\), \\[ \\begin{bmatrix} a_{t + 1} &amp; P_{t + 1} \\end{bmatrix} = \\begin{bmatrix} a_{t + 1,1} &amp; P_{t + 1, 1, 1} &amp; P_{t + 1, 1, 2} &amp; \\cdots &amp; P_{t + 1, 1, m} \\\\ a_{t + 1,2} &amp; P_{t + 1, 2, 1} &amp; P_{t + 1, 2, 2} &amp; \\cdots &amp; P_{t + 1, 2, m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{t + 1,m} &amp; P_{t + 1, m, 1} &amp; P_{t + 1, m, 2} &amp; \\cdots &amp; P_{t + 1, m, m} \\end{bmatrix} . \\] matrix ssm_filter_predict_state_2(...) Given \\(p \\times 1\\) vector \\(\\vec{a}_{t|t} = \\E(\\alpha_t | Y_{t})\\) and \\(\\mat{P}_{t|t} = \\Var(\\alpha_t | Y_{t |t})\\), predict the state in the next period, \\(a_{t+ t} = \\E(\\alpha_{t + 1}| Y_t)\\) and \\(P_{t + t1} = \\Var(\\alpha_{t + 1} | Y_t)\\). Returns a \\(p \\times (1 + p)\\) matrix with \\(a_{t|t}\\) and \\(P_{t|t}\\), in the same format as ssm_filter_predict_state_2. -->"],
["software.html", "Chapter 9 Software ", " Chapter 9 Software "],
["r-packages.html", "9.1 R packages", " 9.1 R packages dse sspir dlm KFAS The function StructTS is included in the recommended stats package. "],
["other.html", "9.2 Other", " 9.2 Other "]
]
