\documentclass{article}

\usepackage{amsmath}
\usepackage{longtable}
\usepackage[backend=biber]{biblatex}
\usepackage{hyperref}

\bibliography{default}

\newcommand{\Rlang}{\textsf{R}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}

\begin{document}

<<echo = FALSE, error = FALSE, results = 'hide'>>=
library("rstan")
library("stringr")
library("dplyr")
library("dlm")
knitr::opts_chunk[["set"]](error = FALSE, cache = TRUE)

rm_attr <- function(x, keep = c("dimnames", "class", "dim")) {
    for (i in setdiff(names(attributes(x)), keep)) {
        attr(x, i) <- NULL
    }
    x
}

@


\section{Model Terminology}

These models are called either state space models (econometrics) or
dynamic (linear) models (statistics / Bayesian).

The following defines a \emph{state space model} 
\begin{equation*}
  \begin{aligned}[t]
    y_t = f(\theta_t \vert  b_{t}, F_t, \nu_t) \\
    \theta_t = f(\theta_{t-1} \vert g_{t},  G_t, \nu_t)
  \end{aligned}
\end{equation*}

If $\theta_t$ is continuous then it is a \emph{continuous state space
model}, if $\theta_t$ is discrete then it is a \emph{discrete state
space model}.

If those equations can be written as 
\begin{equation*}
\begin{aligned}[t]
y_t = b_{t} + F_t \theta_t + \nu_t \\
\theta_t = g_{t} + G_t \theta_{t-1} + \omega_t
\end{aligned}
\end{equation*}
then the model is a \emph{Dynamic Linear Model (DLM)} (linear SSM), otherwise it is a non-linear dynamic model.
If $\nu_t$ and $\omega_t$ are normal distributions, then it is \emph{Guassian} or \emph{Normal Dynamic Linear Model} (GDLM or NDLM). 

A dynamic linear model is defined by the following set of equations,
\begin{align}
  \label{eq:2}
  y_t &= b_{t} + F_t \theta_{t-1} + \nu_t & \nu_t & \sim N(0, V_t) \\
  \label{eq:3}
  \theta_t &= g_{t} + G_t \theta_{t-1} + \omega_t & \omega_t & \sim N(0, W_t) \\
  \label{eq:4}
  \theta_0 &\sim N(m_0, C_0)
\end{align}
where equation \ref{eq:2} is the observation or measurement equation,
equation \ref{eq:3} is the system equation, 
and equation \ref{eq:4} is the initial information.
The number of variables is $r$ and the number of states is $p$.

\begin{table}
  \centering
  \begin{tabular}[c]{@{}ll@{}}
    \hline\noalign{\medskip}
    matrix & dimensions
    \\\noalign{\medskip}
    \hline\noalign{\medskip}
    $F_t$ & $r \times p$
    \\\noalign{\medskip}
    $G_t$ & $p \times p$
    \\\noalign{\medskip}
    $V_t$ & $r \times r$
    \\\noalign{\medskip}
    $W_t$ & $p \times p$
    \\\noalign{\medskip}
    $C_0$ & $p \times p$
    \\\noalign{\medskip}
    \hline
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \begin{tabular}[c]{@{}ll@{}}
    \hline\noalign{\medskip}
    vector & dimensions
    \\\noalign{\medskip}
    \hline\noalign{\medskip}
    $Y_t$ & $r$
    \\\noalign{\medskip}
    $\theta_t$ & $p$
    \\\noalign{\medskip}
    $b_t$ & $r$
    \\\noalign{\medskip}
    $\nu_t$ & $r$
    \\\noalign{\medskip}
    $g_t$ & $p$\
    \\\noalign{\medskip}
    $\omega_t$ & $p$
    \\\noalign{\medskip}
    $m_0$ & $p$
    \\\noalign{\medskip}
    \hline
  \end{tabular}
\end{table}


\subsection{Filtering Equations}

See \textcite[Chapter 2.7, p. 53]{PetrisPetroneEtAl2009} and \textcite[Chapter 4]{WestHarrison1997} for proofs.

Assume the posterior distribution at $t-1$. 
Let $\theta_{t-1} | y_{1:t-1} \sim N(m_{t-1}, C_{t-1})$.
The one step ahead predictive distribution of $\theta_{t}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$,
\begin{align}
  a_{t} &= \E(\theta_{t} | y_{1:t-1}) &&= g_{t} + G_{t} m_{t-1} \\
  R_{t} &= \Var(\theta_{t} | y_{1:t-1}) &&= G_{t} C_{t-1} G'_{t} + W_{t}
\end{align}
The one step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$,
\begin{align}
  f_{t} &= \E(Y_{t} | y_{1:t-1}) &&= b_{t} + F_{t} a_{t} \\
  Q_{t} &= \Var(Y_{t} | y_{1:t-1}) &&= F_{t} R_{t} F'_{t} + V_{t}
\end{align}
The filtered distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
\begin{align}
  \label{eq:5}
  m_{t} &= \E(\theta_{t} | y_{1:t}) &&= a_{t} + R_{t} F'_{t} Q_{t}^{-1} e_{t} \\
  &&& = a_{t} + K_{t} e_{t} \\
  \label{eq:6}
  C_{t} &= \Var(\theta_{t} | y_{1:t}) &&= R_{t} - R_{t} F'_{t} Q_{t}^{-1} F_{t} R_{t} \\
  &&& = R_{t} - K_{t} Q_{t} K'_{t} \\
  \label{eq:1}  
  &&& = (I_{p} - K_{t} F_{t}) R_{t} (I_{p} - K_{t} F_{t})' + K_{t} V_{t} K_{t}' \\
  e_{t} &&&= y_{t} - f_{t} \\
  K_{t} &&&= R_{t} F_{t}' Q_{t}^{-1}
\end{align}
where $e_{t}$ is the one step forecast error and $K_{t}$ is the Kalman gain (adaptative coefficient).
Equation \ref{eq:1} is the ``Joseph stablized form'' \parencite[3]{Tusell2011}.

\begin{table}
  \centering
  \begin{tabular}[]{ll}
    \hline
    variable & dim \\
    \hline
    $a_{t}$ & $p$ \tabularnewline
    $R_{t}$ & $p, p$ \tabularnewline
    $f_{t}$ & $r$ \tabularnewline
    $Q_{t}$ & $r, r$ \tabularnewline
    $m_{t}$ & $p$ \tabularnewline
    $C_{t}$ & $p, p$ \tabularnewline
    $e_{t}$ & $r$ \tabularnewline
    $K_{t}$ & $p, r$ 
  \end{tabular}
  \label{Dimensions of variables in the filtering equations.}
\end{table}

A few key identies \parencite[106-107]{WestHarrison1997} are,
\begin{align}
  K_{t} &= R_{t} F'_{t} Q_{t}^{-1} = C_{t} F'_{t} V_{t}^{-1} \\
  C_{t} &= R_{t}^{-1} - K_{t} Q_{t} K_{t} = R_{t}(I_{p} - F_{t} K_{t}') \\
  C_{t}^{-1} &= R_{t}^{-1} + F_{t}' V_{t}^{-1} F_{t} \\
  Q_{t} &= (I_{r} - F_{t} K_{t})^{-1} V_{t} \\
  F_{t} K_{t} &= I_{r} - V_{t} Q_{t}^{-1}
\end{align}
% Replace $a_{t} = G_{t} m_{t - 1} + \E(\omega_{t})$ and 

\subsection{Missing Values}

Missing values are equivalent to setting $F_{t} = 0$ or $V_{t} = \infty$.

If all values in $t$ are missing, replace the filter steps (equations \ref{eq:5} and \ref{eq:6}) with,
\begin{align}
  m_{t} = a_{t} \\
  C_{t} = R_{t} 
\end{align}

Suppose some, but not all variables in time $t$ are missing.
Let $r_{t} \in [0, r]$ is the number of non-missing values in each time period, $r_{t} = \sum_{j = 1}^{r} (y_{j,t} \neq \emptyset)$.
If some are missing (let $r > r_{t} > 0$ be observed), let $M$ be a $r_{t} \times r$ selection matrix and define,
\begin{align}
  y^{*}_{t} &= M_{t} y_{t} \\
  F^{*}_{t} &= M_{t} F_{t} \\
  V^{*}_{t} &= M_{t} V_{t} M_{t}'
\end{align}
and replace $y_{t}$, $F_{t}$ and $V_{t}$ in the filtering equations with those.
Note that the smoothing (section \ref{sec:smoothing}) and backward sampling (section \ref{sec:backward-sampling}) algorithms do not depend on $F$, $V$ and $y$ and thus do not need to be altered when there are missing values.


\subsection{Likelihood}
\label{sec:likelihood}

If there are no missing values, then the log likelihood is,
\begin{equation}
  L(y_{1:n}) =  - \frac{n r}{2} \log (2 \pi) - \frac{1}{2} \sum_{t=1}^{n}
  \left(
    \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
\end{equation}
If there are missing values,
\begin{equation}
  L(y_{1:n}) = 
  -\frac{1}{2} \sum_{t=1}^{n} 
  (r_{t} > 0) 
  \left(
    r_{t} \log (2 \pi)
    + \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
\end{equation}

See \textcite[Chapter 5, p. 57]{KoopmanShephardDoornik2008}.


\subsection{Smoothing}
\label{sec:smoothing}

If $\theta_{t+1} | y_{1:n} \sim N(s_{t+1}, S_{t+1})$, then $\theta_{t} | y_{1:n} \sim N(s_{t}, S_{t})$ where
\begin{align}
  s_{t} &= \E(\theta_{t} | y_{1:n}) &&= m_{t} + C_{t} G'_{t+1} R_{t+1}^{-1}(s_{t+1} - a_{t+1}) \\
  S_{t} &= \Var(\theta_{t} | y_{1:n}) &&= C_{t} - C_{t} G'_{t+1} R^{-1}_{t+1} (R_{t+1} - S_{t+1}) R^{-1}_{t+1} G_{t+1} C_{t}
\end{align}
Note that $s_{n} = m_{n}$ and $S_{n} = C_{n}$.
See \textcite[Prop 2.4, p. 61]{PetrisPetroneEtAl2009} for a proof.


\subsection{Backward Sampling}
\label{sec:backward-sample}

Supposing that $m_{1:n}$, $C_{1:n}$, $a_{1:n}$ and $R_{1:n}$ have been calculated by the filter,%
\footnote{No additional adjustment for intercepts is required because they are already incorporated in $a_{t}$}
To draw $\theta_{1:n} | y_{1:n}$,

\begin{enumerate}
\item From the Kalman filter, $\theta_{n} | y_{1:n} \sim N(m_{n}, C_{n})$
\item For $t = (n-1), \dots, 0$, $\theta_{t} | y_{1:n} \sim N(h_{t}, H_{t})$ where
  \begin{align}
    h_{t} &= m_{t} + C_{t} G'_{t + 1} R_{t+1}^{-1}(\theta_{t+1} - a_{t+1}) \\
    H_{t} &= C_{t} - C_{t} G'_{t + 1} R_{t+1}^{-1} G_{t+1} C_{t}
  \end{align}
\end{enumerate}
See \textcite[Chapter 4.4.1, p. 161]{PetrisPetroneEtAl2009} for more details.

\section{Sequential Estimation}

First, consider the case in which all $V_{t}$ are diagonal.
The vector series $y_{1}, \dots, y_{n}$ is treated as a scalar series
\begin{equation}
  y_{1,1}, \dots, y_{1,r}, y_{2,1}, \dots, y_{n,r}
\end{equation}

The prior distribution of the state is $\theta_{t} | y_{1:(t-1)} \sim N(a_{t}, R_{t})$,
\begin{align}
  a_{t} &= \E(\theta_{t} | y_{1:t-1}) &&= g_{t} + G_{t} m_{t-1} \\
  R_{t} &= \Var(\theta_{t} | y_{1:t-1}) &&= G_{t} C_{t-1} G'_{t} + W_{t}
\end{align}

For each variable, $i = 1, \dots, r$.
The prediction equation is $y_{t,i} | y_{1:t-1}, y_{t,j|j < i} \sim N(f_{t,i}, Q_{t,i})$.
Note that $f_{t,i}$ and $Q_{t,i}$ are scalars.
\begin{align}
  f_{t,i} &= \E(Y_{t,i} | y_{1:t-1}, y_{t,j | j < i}) &&= b_{t,i} + F_{t,i} m_{t,i-1} \\
  Q_{t,i} &= \Var(Y_{t,i} | y_{1:t-1}, y_{t, j | j < i}) &&= F_{t,i} C_{t,i-1} F'_{t,i} + v_{t,i}
\end{align}
For the first variable, let $m_{t,0} = a_{t}$ and $C_{t,0} = R_{t}$.
The filtered distribution of the latent state is $\theta_{t} | y_{1:t-1}, y_{t,j|j < i} \sim N(m_{t,i}, C_{t,i})$,
\begin{align}
  m_{t,i} &= \E(\theta_{t} | y_{1:t-1}, y_{t, j | j \leq i}) &&= m_{t,i-1} + C_{t,i-1} F'_{t,i} Q_{t,i}^{-1} e_{t,i} \\
  &&& = m_{t,i-1} + K_{t,i} e_{t,i} \\
  C_{t,i} &= \Var(\theta_{t} | y_{1:t-1}, y_{t, j|j \leq i}) &&= C_{t,i-1} - C_{t,i-1} F'_{t,i} Q_{t,i}^{-1} F_{t,i} C_{t,i-1} \\
  &&& = Q_{t,i} - K_{t,i} Q_{t,i} K'_{t,i} \\  
  &&& = (I_{n} - K_{t,i} F_{t,i}) C_{t,i} (I_{n} - K_{t,i} F_{t,i})' + K_{t,i} v_{t,i} K_{t,i}' \\
  e_{t,i} &&&= y_{t,i} - f_{t,i} \\
  K_{t,i} &&&= C_{t,i-i} F_{t,i}' Q_{t,i}^{-1}
\end{align}
Define $m_{t} = m_{t,r}$ and $C_{t} = C_{t,r}$.
If $y_{t,i}$ is missing, then 
\begin{align}
  m_{t,i} &= m_{t,i-1} \\
  C_{t,i} &= C_{t,i-1}
\end{align}

The likelihood in the sequential case is
\begin{equation}
  L(y_{1:n}) = -\frac{1}{2} \sum_{t=1}^{n} \sum_{i=1}^{r} (y_{i,j} \neq \emptyset)
  \left( 
    \log (2 \pi) + \log |Q_{t,i}| + \frac{e_{t,i}^{2}}{Q_{t,i}} 
  \right)
\end{equation}

If $V_{t}$ is not diagonal, then diagonalize it with the Cholesky decomposition of $V_{t}$, such that $V_{t} = L_{t} D_{t} L'_{t}$ where $D_{t}$ is diagonal, and $L_{t}$ is lower triangular.
\begin{align}
  y_{t}^{*} &= L_{t}^{-1} y_{t} \\
  F_{t}^{*} &= L_{t}^{-1} F_{t} \\
  b_{t}^{*} &= L_{t}^{-1} b_{t} \\
  \epsilon^{*} & = L_{t}^{-1} \epsilon_{t}  \sim N(0, D_{t})
\end{align}

%% \subsection{Discounting}
%% \label{sec:smoothing-1}

%% See \textcite[Chapter 6.3]{WestHarrison1997}

%% Let $\Delta$ be covariance matrix, with the entries in the diagonal between 0 and 1.

%% \begin{itemize}
%% \item Component discounting (West and Harrison)
%%   \begin{equation*}
%%     W_{t} = \frac{1 - \delta}{\delta} G_{t} C_{t - 1} G_{t}'
%%   \end{equation*}
%%   Let $R_{t} = \Delta^{-1} P_{t}$ and $R_{t} = P_{t} + W_{t}$ where $P_{t} = G_{t} C_{t-1} G_{t}'$,
%%   \begin{equation*}
%%     W_{t} = (\Delta^{-1} - I) P_{t} 
%%   \end{equation*}
%% \item Let $R_{t} = \Delta G_{t} C_{t-1} G'_{t} \Delta$ (p. 202)
%% \item Let $R_{t} = G_{t} \Delta C_{t-1} \Delta G'_{t}$ (p. 202)
%% \end{itemize}


\subsection{Univariate Local Level Model}
\label{sec:local-level-model}

For the univariate local level model, with $r = n = 1$, $F_{t} = G_{t} = 1$,
and $g_{t} = b_{t} = 0$, the calculations can be simplified.

One step ahead predictive distribution of $\theta_{t-1}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$
\begin{align*}
  a_{t} &= E(\theta_{t} | y_{1:t-1}) = m_{t-1} \\
  R_{t} &= Var(\theta_{t} | y_{1:t-1}) = C_{t-1} + W_{t}
\end{align*}

The One step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$
\begin{align*}
  f_{t} &= E(Y_{t} | y_{1:t-1}) = a_{t} = m_{t-1} \\
  Q_{t} &= Var(Y_{t} | y_{1:t-1}) = R_{t} + V_{t} = C_{t-1} + W_{t} + V_{t}
\end{align*}

The posterior distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
\begin{align*}
  f_{t} &= E(\theta_{t} | y_{1:t}) = a_{t} + R_{t} Q_{t}^{-1} e_{t} = m_{t-1} + \frac{C_{t-1} + W_{t}}{C_{t-1} + W_{t} + V_{t}} e_{t} \\
  Q_{t} &= Var(\theta_{t} | y_{1:t}) = R_{t} - \frac{(C_{t-1} + W_{t})^{2}}{C_{t-1} + W_{t} + V_{t}}
\end{align*}
where $e_{t} = Y_{t} - f_{t}$. The Kalman gain is defined as $K_{t} = R_{t} / Q_{t}$.



\section{Notes}

\begin{itemize}
\item Use discounting
  \begin{itemize}
  \item Use a beta rectangular distribution
  \item Beta distribution with priors over $\alpha$  and $\beta$
  \end{itemize}
\end{itemize}



\section{Examples}


\subsection{Nile}

The data is included with \Rlang{} as \texttt{datasets::Nile}.

\begin{enumerate}
\item Standard
\item With missing
\item With intervention
\end{enumerate}

\begin{align}
  y_{t} &\sim N(\theta_{t}, V) \\
  \theta_{t} &\sim N(\theta_{t - 1}, W)
\end{align}
where $r = p = 1$, $F = G = 1$, and $b = g = 0$.

The stan model is as follows
<<comment="">>=
cat(str_c(readLines("stan/kalman_batch.stan"), sep = "\n"))

@ 

<<>>=
data("Nile")
y <- as.numeric(Nile)

standata <- 
    within(list(), {
        y <- matrix(y)
        n <- length(y)
        r <- 1L
        p <- 1L
        b <- 0
        g <- 0
        F <- matrix(1, 1, 1)
        G <- matrix(1, 1, 1)
        m0 <- array(0, 1)
        C0 <- matrix(1000, 1, 1)
        })

m <- stan_model("stan/kalman_batch.stan")
ret <- sampling(m, data = standata)

apply(extract(ret, "V")[[1]], 2, mean)
apply(extract(ret, "W")[[1]], 2, mean)
mean(extract(ret, "loglik")[[1]])

@ 



\subsection{UK Gas}

This is a model of quarterly consumption of gas in the UK from 1960 to 1986.
This data is included with \Rlang{} as \texttt{datasets::UKGas}.
The model to be estimated is a local linear trend model and a quarterly seasonal factor model.
\begin{equation*}
  \begin{aligned}[t]
    F &=
    \begin{bmatrix}
      1 & 0 & 1 & 0 & 0 
    \end{bmatrix}
    & 
    G &=
    \begin{bmatrix}
      1 & 1 & 0 & 0 & 0  \\
      0 & 1 & -1 & -1 & -1 \\
      0 & 0 & 1 & 0 & 0 \\
      0 & 0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 0 & 1
    \end{bmatrix}
    \\
    V & = \sigma_{y}^{2} &
    W & = \diag (0, \sigma_{\beta}^{2}, \sigma_{s}^{2}, 0, 0) 
  \end{aligned}
\end{equation*}
Thus $r = 1$, $p = 1$.

<<>>=
data("UKgas")
y <- log(as.numeric(UKgas))

standata <- 
    within(list(), {
        y <- matrix(y)
        n <- length(y)
        r <- 1
        p <- 5
        b <- array(0, r)
        g <- array(0, p)
        F <- matrix(c(1, 0, 1, 0, 0), r, p)
        G <- matrix(c(1, 1,  0,  0,  0,
                      0, 1,  0,  0,  0,
                      0, 0,  -1, -1, -1,
                      0, 0,  1,  0,  0,                      
                      0, 0,  0,  1,  0),
                    p, p,
                    byrow = TRUE)
        m0 <- array(0, p)
        C0 <- diag(p) * 1e7
    })

m <- stan_model("stan/ukgas.stan")
ret <- sampling(m, data = standata, chains = 1, iter = 10000)

apply(extract(ret, "V")[[1]], 2, median)
apply(extract(ret, "sigma2")[[1]], 2, median)
theta <- apply(extract(ret, "theta")[[1]], 2:3, mean)
plot(theta[ , 1][4:109], type = "l")
plot(theta[ , 3][4:109], type = "l")
plot((theta[ , 1] + theta[ , 3])[4:109], type = "l")

s <- apply(extract(ret, "s")[[1]], 2:3, mean)
plot(s[ , 1], type = "l")
plot(s[ , 3], type = "l")

thetam <- apply(extract(ret, "m")[[1]], 2:3, mean)
plot(thetam[ , 1], type = "l")
plot(thetam[ , 3], type = "l")

thetaa <- apply(extract(ret, "a")[[1]], 2:3, mean)
plot(thetaa[ , 1], type = "l")
plot(thetaa[ , 3], type = "l")

plot(y, type = "l")

@ 


<<>>=
dlmGas <- dlmModPoly() + dlmModSeas(4)
buildFun <- function(x) {
    diag(W(dlmGas))[2:3] <- exp(x[1:2])
    V(dlmGas) <- exp(x[3])
    return(dlmGas)
}
(fit <- dlmMLE(y, parm = rep(0, 3), build = buildFun))$conv
dlmGas <- buildFun(fit$par)
drop(V(dlmGas))
diag(W(dlmGas))[2:3]

@ 

\subsection{Temperature}


<<>>=
library("dlm")
buildTemp <- function(x) {
  L <- matrix(0, 2, 2)
  L[upper.tri(L, TRUE)] <- x[1 : 3]
  diag(L) <- exp(diag(L))
  modTemp <- dlm(FF = matrix(1, 2, 1), V = crossprod(L),
  GG = 1, W = exp(x[4]), m0 = 0, C0 = 1e7)
  return(modTemp)
}
y1 <- scan("http://www.stat.pitt.edu/stoffer/tsa2/data/HL.dat")
y2 <- scan("http://www.stat.pitt.edu/stoffer/tsa2/data/Folland.dat")
y <- cbind(y1, y2)
fitTemp <- dlmMLE(y, parm = rep(0, 4), build = buildTemp,
                  hessian = TRUE, control = list(maxit = 500))
estTemp <- buildTemp(fitTemp$par)

@ 

<<>>=

temp_data <-
    within(list(), {
        y <- y
        n <- nrow(y)
        r <- ncol(y)
        p <- 1
        F <- matrix(1, r, p)
        G <- matrix(1, p, p)
        b <- array(0, r)
        g <- array(0, p)
        m0 <- array(0, p)
        C0 <- diag(p) * 1e7
    })

temp_model <- stan_model("stan/temp.stan")
temp_est <- sampling(temp_model, data = temp_data, chains = 1, iter = 500)

apply(extract(ret, "V")[[1]], 2:3, mean)
apply(extract(ret, "W")[[1]], 2:3, mean)

temp_data2 <- temp_data
temp_data2$y <- t(temp_data2$y)
temp2_model <- stan_model("stan/temp2.stan")
temp2_est <- sampling(temp2_model, data = temp_data2, chains = 1, iter = 500)


@ 



\subsection{Foreign Exchange}

\subsubsection{Equicorrelated}

The first model is a local level model with $V_{t}$ equicorrelated.
\begin{equation}
  y_{t} &= I_{r} \theta_{t} + V \\
  \theta_{t} &= I_{r} \theta_{t-1} + W
  V_{t,i,j} &= 
  \begin{cases}
    \sigma_{i,j}^{2} & \text{if $i = j$} \\
    \sigma_{i} \sigma_{j} \rho & \text{if $i \neq j$}
  \end{cases} \\
  W &= \diag(w)
\end{equation}

Right now the following code is taking a very long time ....
<<>>=
divisas <- 
    mutate(read.csv("data/divisas.csv"),
           date = as.Date(date, format = "%Y-%m-%d"))

y <- rm_attr(scale(log(as.matrix(divisas[ , c("BEF", "CHF", "DEM")])),
                   center = TRUE, scale = FALSE),
             keep = "dim")

standata <-
    within(list(), {
        y <- y
        r <- ncol(y)
        n <- nrow(y)
        p <- r
        b <- array(0, r)
        g <- array(0, p)
        F <- diag(r)
        G <- diag(p)
        m0 <- array(0, r)
        C0 <- diag(r) * 1e7
    })

fxequicorr_model <- stan_model("stan/fxequicorr.stan")
fxequicorr_est <- sampling(fxequicorr_model, data = standata, chains = 1, iter = 50)

thetam <- apply(extract(ret, "m")[[1]], 2:3, mean)
plot(thetam[ , 1], type = "l")
plot(thetam[ , 2], type = "l")
plot(thetam[ , 3], type = "l")

theta <- apply(extract(ret, "theta")[[1]], 2:3, mean)
plot(thetam[ , 1], type = "l")
plot(thetam[ , 2], type = "l")
plot(thetam[ , 3], type = "l")

@ 



<<>>=
nv  <- ncol(y)
y  <- as.matrix(log(divisas[ , c("BEF", "CHF", "DEM")]))
C0 <- diag(nv)*10^5
m0 <- matrix(0,1,nv)
InitialValues <- InitialValues <- c(rep(0.000002,2*nv),0.5)
lowerlimits <- c(rep(0.000001,2*nv),-0.99)
upperlimits <- c(rep(Inf,2*nv),+0.99)


equicorrel <- function(nv,vars,rho) {
  sd <- sqrt(vars)
  m1 <- sd %o% sd
  m2 <- matrix(rho,nv,nv)
  diag(m2) <- 1
  return(m1*m2)
}

buildFun <- function(parm) {
  nv <- nv
  Vvars <- parm[1:nv]
  Wvars <- parm[(nv+1):(2*nv)]
  rho  <-  parm[2*nv+1] 
  y    <- y
  dlm(FF = diag(nv), GG=diag(nv), 
      W=diag(Wvars), V=equicorrel(nv,Vvars,rho),
      m0=as.vector(m0), C0=C0)
}

Curr2.dlm.fit <- dlmMLE(y, parm = InitialValues, method="L-BFGS-B",
                        lower=lowerlimits,upper=upperlimits,
                        build = buildFun,control=list(parscale=InitialValues))
Curr2.dlm.estimated <- buildFun(Curr2.dlm.fit$par)

@ 

\subsubsection{1-Factor Model}

A one-factor model of the foreign exchange rates.
\begin{align}
  y_{t,j} &= \theta_{t} + \nu_{t,j} & \nu_{t,j} & \sim N(0, \sigma_{j}^{2}) \\
  \theta_{t}&= \theta_{t-1} + \omega_{t,j} & \omega_{t} & \sim N(0, W) 
\end{align}

<<>>=
standata <- 
    within(list(), {
        y <- rm_attr(scale(log(as.matrix(divisas[ , c("BEF", "CHF", "DEM")])),
                           center = TRUE, scale = TRUE),
                     keep = "dim")
        y_obs <- matrix(sample(0:1, length(y), replace = TRUE, prob = c(0.33, 0.67)),
                        nrow(y), ncol(y))
        Qfill <- 1e7
        r <- ncol(y)
        n <- nrow(y)
        p <- 1
        b <- array(0, r)
        g <- array(0, p)
        F <- matrix(1, r, p)
        G <- diag(p)
        m0 <- array(0, p)
        C0 <- diag(p) * 1e7
    })

fx1factor_model <- stan_model("stan/fx1factor.stan")
fx1factor_model <- stan_model("stan/kalman_seq_vw_miss.stan")
fx1factor_est <- sampling(fx1factor_model, data = standata, chains = 1, iter = 100)

apply(extract(fx1factor_est, "V")[[1]], 2, median)
apply(extract(fx1factor_est, "W")[[1]], 2, median)
theta <- apply(extract(fx1factor_est, "theta")[[1]], 2:3, mean)
plot(theta[ -1, 1], type = "l")
thetam <- apply(extract(fx1factor_est, "m")[[1]], 2:3, mean)
plot(thetam[ , 1], type = "l")

fx1factor2_model <- stan_model("stan/fx1factor2.stan")
fx1factor2_est <- sampling(fx1factor2_model, data = within(standata, y <- t(y)), chains = 1, iter = 200)
apply(extract(fx1factor2_est, "V")[[1]], 2, median)
apply(extract(fx1factor2_est, "W")[[1]], 2, median)


@ 

<<>>=
M <- matrix(c(1, 0, 0, 0, 
              0, 0, 1, 0,
              0, 0, 0, 0,
              0, 0, 0, 0),
            4, 4, byrow = TRUE)


missing_matrix <- function(x) {
    n <- length(x)
    nonmiss <- !is.na(miss)
    xrow <- cumsum(nonmiss)[nonmiss]
    xcol <- which(nonmiss)
    M <- matrix(0, n, n)
    M[cbind(xrow, xcol)] <- 1
    M
}

@ 


\subsection{Industrial Production}

A linear trend model of U.S. industrial production of consumption goods, seasonally adjusted.
The data is \href{http://research.stlouisfed.org/fred2/series/IPCONGD}{IPCONGD}; also available on \href{http://www.quandl.com/FRED/IPCONGD}{Quandl}.

\begin{equation*}
  \begin{aligned}[t]
    F &=
    \begin{bmatrix}
      1 & 0 
    \end{bmatrix}
    & 
    G &=
    \begin{bmatrix}
      1 & 1 \\
      0 & 1 
    \end{bmatrix}
    \\
    V & = \sigma_{y}^{2} &
    W & = \diag (\sigma_{1}^{2}, \sigma_{2}^{2})
  \end{aligned}
\end{equation*}

\subsection{CAPM}

The state space are the betas of the stocks,
\begin{equation}
  \theta_{t} = (\beta_{1,t}, \dots, \beta_{m,t})'
\end{equation}
The system equations are,
\begin{equation}
  \begin{aligned}[t]
    F_{t} &= x_{t} I & G_{t} &= I \\
    V_{t} &= \Sigma_{\epsilon} & W_{t} &= \Sigma_{\beta}
  \end{aligned}
\end{equation}



\printbibliography{}

\end{document}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
