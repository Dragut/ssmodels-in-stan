\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{amsmath}
\usepackage{longtable}
\usepackage[backend=biber]{biblatex}
\usepackage{hyperref}

\bibliography{default}

\newcommand{\Rlang}{\textsf{R}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: Rcpp\\\#\# Loading required package: inline\\\#\# Loading required package: methods\\\#\# \\\#\# Attaching package: 'inline'\\\#\# \\\#\# The following object is masked from 'package:Rcpp':\\\#\# \\\#\#\ \ \ \  registerPlugin\\\#\# \\\#\# rstan (Version 2.2.0, packaged: 2014-05-13 20:40:04 UTC)}}\end{kframe}
\end{knitrout}


\section{Model Terminology}

These models are called either state space models (econometrics) or
dynamic (linear) models (statistics / Bayesian).

The following defines a \emph{state space model} 
\begin{equation*}
  \begin{aligned}[t]
    y_t = f(\theta_t \vert  b_{t}, F_t, \nu_t) \\
    \theta_t = f(\theta_{t-1} \vert g_{t},  G_t, \nu_t)
  \end{aligned}
\end{equation*}

If $\theta_t$ is continuous then it is a \emph{continuous state space
model}, if $\theta_t$ is discrete then it is a \emph{discrete state
space model}.

If those equations can be written as 
\begin{equation*}
\begin{aligned}[t]
y_t = b_{t} + F_t \theta_t + \nu_t \\
\theta_t = g_{t} + G_t \theta_{t-1} + \omega_t
\end{aligned}
\end{equation*}
then the model is a \emph{Dynamic Linear Model (DLM)} (linear SSM), otherwise it is a non-linear dynamic model.
If $\nu_t$ and $\omega_t$ are normal distributions, then it is \emph{Guassian} or \emph{Normal Dynamic Linear Model} (GDLM or NDLM). 

A dynamic linear model is defined by the following set of equations,
\begin{align}
  \label{eq:2}
  y_t &= b_{t} + F_t \theta_{t-1} + \nu_t & \nu_t & \sim N(0, V_t) \\
  \label{eq:3}
  \theta_t &= g_{t} + G_t \theta_{t-1} + \omega_t & \omega_t & \sim N(0, W_t) \\
  \label{eq:4}
  \theta_0 &\sim N(m_0, C_0)
\end{align}
where equation \ref{eq:2} is the observation or measurement equation,
equation \ref{eq:3} is the system equation, 
and equation \ref{eq:4} is the initial information.
The number of variables is $r$ and the number of states is $p$.

\begin{table}
  \centering
  \begin{tabular}[c]{@{}ll@{}}
    \hline\noalign{\medskip}
    matrix & dimensions
    \\\noalign{\medskip}
    \hline\noalign{\medskip}
    $F_t$ & $r \times p$
    \\\noalign{\medskip}
    $G_t$ & $p \times p$
    \\\noalign{\medskip}
    $V_t$ & $r \times r$
    \\\noalign{\medskip}
    $W_t$ & $p \times p$
    \\\noalign{\medskip}
    $C_0$ & $p \times p$
    \\\noalign{\medskip}
    \hline
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \begin{tabular}[c]{@{}ll@{}}
    \hline\noalign{\medskip}
    vector & dimensions
    \\\noalign{\medskip}
    \hline\noalign{\medskip}
    $Y_t$ & $r$
    \\\noalign{\medskip}
    $\theta_t$ & $p$
    \\\noalign{\medskip}
    $b_t$ & $r$
    \\\noalign{\medskip}
    $\nu_t$ & $r$
    \\\noalign{\medskip}
    $g_t$ & $p$\
    \\\noalign{\medskip}
    $\omega_t$ & $p$
    \\\noalign{\medskip}
    $m_0$ & $p$
    \\\noalign{\medskip}
    \hline
  \end{tabular}
\end{table}


\subsection{Filtering Equations}

See \textcite[Chapter 2.7, p. 53]{PetrisPetroneEtAl2009} and \textcite[Chapter 4]{WestHarrison1997} for proofs.

Assume the posterior distribution at $t-1$. 
Let $\theta_{t-1} | y_{1:t-1} \sim N(m_{t-1}, C_{t-1})$.
The one step ahead predictive distribution of $\theta_{t}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$,
\begin{align}
  a_{t} &= \E(\theta_{t} | y_{1:t-1}) &&= g_{t} + G_{t} m_{t-1} \\
  R_{t} &= \Var(\theta_{t} | y_{1:t-1}) &&= G_{t} C_{t-1} G'_{t} + W_{t}
\end{align}
The one step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$,
\begin{align}
  f_{t} &= \E(Y_{t} | y_{1:t-1}) &&= b_{t} + F_{t} a_{t} \\
  Q_{t} &= \Var(Y_{t} | y_{1:t-1}) &&= F_{t} R_{t} F'_{t} + V_{t}
\end{align}
The filtered distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
\begin{align}
  \label{eq:5}
  m_{t} &= \E(\theta_{t} | y_{1:t}) &&= a_{t} + R_{t} F'_{t} Q_{t}^{-1} e_{t} \\
  &&& = a_{t} + K_{t} e_{t} \\
  \label{eq:6}
  C_{t} &= \Var(\theta_{t} | y_{1:t}) &&= R_{t} - R_{t} F'_{t} Q_{t}^{-1} F_{t} R_{t} \\
  &&& = R_{t} - K_{t} Q_{t} K'_{t} \\
  \label{eq:1}  
  &&& = (I_{p} - K_{t} F_{t}) R_{t} (I_{p} - K_{t} F_{t})' + K_{t} V_{t} K_{t}' \\
  e_{t} &&&= y_{t} - f_{t} \\
  K_{t} &&&= R_{t} F_{t}' Q_{t}^{-1}
\end{align}
where $e_{t}$ is the one step forecast error and $K_{t}$ is the Kalman gain (adaptative coefficient).
Equation \ref{eq:1} is the ``Joseph stablized form'' \parencite[3]{Tusell2011}.

\begin{table}
  \centering
  \begin{tabular}[]{ll}
    \hline
    variable & dim \\
    \hline
    $a_{t}$ & $p$ \tabularnewline
    $R_{t}$ & $p, p$ \tabularnewline
    $f_{t}$ & $r$ \tabularnewline
    $Q_{t}$ & $r, r$ \tabularnewline
    $m_{t}$ & $p$ \tabularnewline
    $C_{t}$ & $p, p$ \tabularnewline
    $e_{t}$ & $r$ \tabularnewline
    $K_{t}$ & $p, r$ 
  \end{tabular}
  \label{Dimensions of variables in the filtering equations.}
\end{table}

A few key identies \parencite[106-107]{WestHarrison1997} are,
\begin{align}
  K_{t} &= R_{t} F'_{t} Q_{t}^{-1} = C_{t} F'_{t} V_{t}^{-1} \\
  C_{t} &= R_{t}^{-1} - K_{t} Q_{t} K_{t} = R_{t}(I_{p} - F_{t} K_{t}') \\
  C_{t}^{-1} &= R_{t}^{-1} + F_{t}' V_{t}^{-1} F_{t} \\
  Q_{t} &= (I_{r} - F_{t} K_{t})^{-1} V_{t} \\
  F_{t} K_{t} &= I_{r} - V_{t} Q_{t}^{-1}
\end{align}
% Replace $a_{t} = G_{t} m_{t - 1} + \E(\omega_{t})$ and 

\subsection{Missing Values}

Missing values are equivalent to setting $F_{t} = 0$ or $V_{t} = \infty$.

If all values in $t$ are missing, replace the filter steps (equations \ref{eq:5} and \ref{eq:6}) with,
\begin{align}
  m_{t} = a_{t} \\
  C_{t} = R_{t} 
\end{align}

Suppose some, but not all variables in time $t$ are missing.
Let $r_{t} \in [0, r]$ is the number of non-missing values in each time period, $r_{t} = \sum_{j = 1}^{r} (y_{j,t} \neq \emptyset)$.
If some are missing (let $r > r_{t} > 0$ be observed), let $M$ be a $r_{t} \times r$ selection matrix and define,
\begin{align}
  y^{*}_{t} &= M_{t} y_{t} \\
  F^{*}_{t} &= M_{t} F_{t} \\
  V^{*}_{t} &= M_{t} V_{t} M_{t}'
\end{align}
and replace $y_{t}$, $F_{t}$ and $V_{t}$ in the filtering equations with those.
Note that the smoothing (section \ref{sec:smoothing}) and backward sampling (section \ref{sec:backward-sampling}) algorithms do not depend on $F$, $V$ and $y$ and thus do not need to be altered when there are missing values.


\subsection{Likelihood}
\label{sec:likelihood}

If there are no missing values, then the log likelihood is,
\begin{equation}
  L(y_{1:n}) =  - \frac{n r}{2} \log (2 \pi) - \frac{1}{2} \sum_{t=1}^{n}
  \left(
    \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
\end{equation}
If there are missing values,
\begin{equation}
  L(y_{1:n}) = 
  -\frac{1}{2} \sum_{t=1}^{n} 
  (r_{t} > 0) 
  \left(
    r_{t} \log (2 \pi)
    + \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
\end{equation}

See \textcite[Chapter 5, p. 57]{KoopmanShephardDoornik2008}.


\subsection{Smoothing}
\label{sec:smoothing}

If $\theta_{t+1} | y_{1:n} \sim N(s_{t+1}, S_{t+1})$, then $\theta_{t} | y_{1:n} \sim N(s_{t}, S_{t})$ where
\begin{align}
  s_{t} &= \E(\theta_{t} | y_{1:n}) &&= m_{t} + C_{t} G'_{t+1} R_{t+1}^{-1}(s_{t+1} - a_{t+1}) \\
  S_{t} &= \Var(\theta_{t} | y_{1:n}) &&= C_{t} - C_{t} G'_{t+1} R^{-1}_{t+1} (R_{t+1} - S_{t+1}) R^{-1}_{t+1} G_{t+1} C_{t}
\end{align}
Note that $s_{n} = m_{n}$ and $S_{n} = C_{n}$.
See \textcite[Prop 2.4, p. 61]{PetrisPetroneEtAl2009} for a proof.


\subsection{Backward Sampling}
\label{sec:backward-sample}

Supposing that $m_{1:n}$, $C_{1:n}$, $a_{1:n}$ and $R_{1:n}$ have been calculated by the filter,%
\footnote{No additional adjustment for intercepts is required because they are already incorporated in $a_{t}$}
To draw $\theta_{1:n} | y_{1:n}$,

\begin{enumerate}
\item From the Kalman filter, $\theta_{n} | y_{1:n} \sim N(m_{n}, C_{n})$
\item For $t = (n-1), \dots, 0$, $\theta_{t} | y_{1:n} \sim N(h_{t}, H_{t})$ where
  \begin{align}
    h_{t} &= m_{t} + C_{t} G'_{t + 1} R_{t+1}^{-1}(\theta_{t+1} - a_{t+1}) \\
    H_{t} &= C_{t} - C_{t} G'_{t} R_{t+1}^{-1} G_{t+1} C_{t}
  \end{align}
\end{enumerate}
See \textcite[Chapter 4.4.1, p. 161]{PetrisPetroneEtAl2009} for more details.

\section{Sequential Estimation}

First, consider the case in which all $V_{t}$ are diagonal.
The vector series $y_{1}, \dots, y_{n}$ is treated as a scalar series
\begin{equation}
  y_{1,1}, \dots, y_{1,r}, y_{2,1}, \dots, y_{n,r}
\end{equation}

The prior distribution of the state is $\theta_{t} | y_{1:(t-1)} \sim N(a_{t}, R_{t})$,
\begin{align}
  a_{t} &= \E(\theta_{t} | y_{1:t-1}) &&= g_{t} + G_{t} m_{t-1} \\
  R_{t} &= \Var(\theta_{t} | y_{1:t-1}) &&= G_{t} C_{t-1} G'_{t} + W_{t}
\end{align}

For each variable, $i = 1, \dots, r$.
The prediction equation is $y_{t,i} | y_{1:t-1}, y_{t,j|j < i} \sim N(f_{t,i}, Q_{t,i})$.
Note that $f_{t,i}$ and $Q_{t,i}$ are scalars.
\begin{align}
  f_{t,i} &= \E(Y_{t,i} | y_{1:t-1}, y_{t,j | j < i}) &&= b_{t,i} + F_{t,i} m_{t,i-1} \\
  Q_{t,i} &= \Var(Y_{t,i} | y_{1:t-1}, y_{t, j | j < i}) &&= F_{t,i} C_{t,i-1} F'_{t,i} + v_{t,i}
\end{align}
For the first variable, let $m_{t,0} = a_{t}$ and $C_{t,0} = R_{t}$.
The filtered distribution of the latent state is $\theta_{t} | y_{1:t-1}, y_{t,j|j < i} \sim N(m_{t,i}, C_{t,i})$,
\begin{align}
  m_{t,i} &= \E(\theta_{t} | y_{1:t-1}, y_{t, j | j \leq i}) &&= m_{t,i-1} + C_{t,i-1} F'_{t,i} Q_{t,i}^{-1} e_{t,i} \\
  &&& = m_{t,i-1} + K_{t,i} e_{t,i} \\
  C_{t,i} &= \Var(\theta_{t} | y_{1:t-1}, y_{t, j|j \leq i}) &&= C_{t,i-1} - C_{t,i-1} F'_{t,i} Q_{t,i}^{-1} F_{t,i} C_{t,i-1} \\
  &&& = Q_{t,i} - K_{t,i} Q_{t,i} K'_{t,i} \\  
  &&& = (I_{n} - K_{t,i} F_{t,i}) C_{t,i} (I_{n} - K_{t,i} F_{t,i})' + K_{t,i} v_{t,i} K_{t,i}' \\
  e_{t,i} &&&= y_{t,i} - f_{t,i} \\
  K_{t,i} &&&= C_{t,i-i} F_{t,i}' Q_{t,i}^{-1}
\end{align}
Define $m_{t} = m_{t,r}$ and $C_{t} = C_{t,r}$.
If $y_{t,i}$ is missing, then 
\begin{align}
  m_{t,i} &= m_{t,i-1} \\
  C_{t,i} &= C_{t,i-1}
\end{align}

The likelihood in the sequential case is
\begin{equation}
  L(y_{1:n}) = -\frac{1}{2} \sum_{t=1}^{n} \sum_{i=1}^{r} (y_{i,j} \neq \emptyset)
  \left( 
    \log (2 \pi) + \log |Q_{t,i}| + \frac{e_{t,i}^{2}}{Q_{t,i}} 
  \right)
\end{equation}

If $V_{t}$ is not diagonal, then diagonalize it with the Cholesky decomposition of $V_{t}$, such that $V_{t} = L_{t} D_{t} L'_{t}$ where $D_{t}$ is diagonal, and $L_{t}$ is lower triangular.
\begin{align}
  y_{t}^{*} &= L_{t}^{-1} y_{t} \\
  F_{t}^{*} &= L_{t}^{-1} F_{t} \\
  b_{t}^{*} &= L_{t}^{-1} b_{t} \\
  \epsilon^{*} & = L_{t}^{-1} \epsilon_{t}  \sim N(0, D_{t})
\end{align}

%% \subsection{Discounting}
%% \label{sec:smoothing-1}

%% See \textcite[Chapter 6.3]{WestHarrison1997}

%% Let $\Delta$ be covariance matrix, with the entries in the diagonal between 0 and 1.

%% \begin{itemize}
%% \item Component discounting (West and Harrison)
%%   \begin{equation*}
%%     W_{t} = \frac{1 - \delta}{\delta} G_{t} C_{t - 1} G_{t}'
%%   \end{equation*}
%%   Let $R_{t} = \Delta^{-1} P_{t}$ and $R_{t} = P_{t} + W_{t}$ where $P_{t} = G_{t} C_{t-1} G_{t}'$,
%%   \begin{equation*}
%%     W_{t} = (\Delta^{-1} - I) P_{t} 
%%   \end{equation*}
%% \item Let $R_{t} = \Delta G_{t} C_{t-1} G'_{t} \Delta$ (p. 202)
%% \item Let $R_{t} = G_{t} \Delta C_{t-1} \Delta G'_{t}$ (p. 202)
%% \end{itemize}


\subsection{Univariate Local Level Model}
\label{sec:local-level-model}

For the univariate local level model, with $r = n = 1$, $F_{t} = G_{t} = 1$,
and $g_{t} = b_{t} = 0$, the calculations can be simplified.

One step ahead predictive distribution of $\theta_{t-1}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$
\begin{align*}
  a_{t} &= E(\theta_{t} | y_{1:t-1}) = m_{t-1} \\
  R_{t} &= Var(\theta_{t} | y_{1:t-1}) = C_{t-1} + W_{t}
\end{align*}

The One step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$
\begin{align*}
  f_{t} &= E(Y_{t} | y_{1:t-1}) = a_{t} = m_{t-1} \\
  Q_{t} &= Var(Y_{t} | y_{1:t-1}) = R_{t} + V_{t} = C_{t-1} + W_{t} + V_{t}
\end{align*}

The posterior distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
\begin{align*}
  f_{t} &= E(\theta_{t} | y_{1:t}) = a_{t} + R_{t} Q_{t}^{-1} e_{t} = m_{t-1} + \frac{C_{t-1} + W_{t}}{C_{t-1} + W_{t} + V_{t}} e_{t} \\
  Q_{t} &= Var(\theta_{t} | y_{1:t}) = R_{t} - \frac{(C_{t-1} + W_{t})^{2}}{C_{t-1} + W_{t} + V_{t}}
\end{align*}
where $e_{t} = Y_{t} - f_{t}$. The Kalman gain is defined as $K_{t} = R_{t} / Q_{t}$.



\section{Notes}

\begin{itemize}
\item Use discounting
  \begin{itemize}
  \item Use a beta rectangular distribution
  \item Beta distribution with priors over $\alpha$  and $\beta$
  \end{itemize}
\end{itemize}



\section{Examples}


\subsection{Nile}

The data is included with \Rlang{} as \texttt{datasets::Nile}.

\begin{enumerate}
\item Standard
\item With missing
\item With intervention
\end{enumerate}

\begin{align}
  y_{t} &\sim N(\theta_{t}, V) \\
  \theta_{t} &\sim N(\theta_{t - 1}, W)
\end{align}
where $r = p = 1$, $F = G = 1$, and $b = g = 0$.

The stan model is as follows
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{str_c}\hlstd{(}\hlkwd{readLines}\hlstd{(}\hlstr{"stan/kalman_batch.stan"}\hlstd{),} \hlkwc{sep} \hlstd{=} \hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
 [1] "data {"                                                                  
 [2] "  // dimensions"                                                         
 [3] "  int n; // number of observations"                                      
 [4] "  int r; // number of variables"                                         
 [5] "  int p; // number of states"                                            
 [6] "  // observations"                                                       
 [7] "  vector[r] y[n];"                                                       
 [8] "  // system matrices"                                                    
 [9] "  // observation equation"                                               
[10] "  matrix[r, p] F;"                                                       
[11] "  real b;"                                                               
[12] "  // system equation"                                                    
[13] "  matrix[p, p] G;"                                                       
[14] "  real g;"                                                               
[15] "  // initial conditions"                                                 
[16] "  vector[p] m0;"                                                         
[17] "  cov_matrix[p] C0;"                                                     
[18] "}"                                                                       
[19] "transformed data {"                                                      
[20] "  matrix[p, p] Ip;"                                                      
[21] "  {"                                                                     
[22] "    vector[p] Ip_vector;"                                                
[23] "    Ip_vector <- rep_vector(1, p);"                                      
[24] "    Ip <- diag_matrix(Ip_vector);"                                       
[25] "  }"                                                                     
[26] "}"                                                                       
[27] "parameters {"                                                            
[28] "  cov_matrix[r] V;"                                                      
[29] "  cov_matrix[p] W;"                                                      
[30] "}"                                                                       
[31] "transformed parameters {"                                                
[32] "  // log-likelihood"                                                     
[33] "  real loglik_obs[n];"                                                   
[34] "  real loglik;"                                                          
[35] "  // prior of state: p(theta_t | y_t, ..., y_{t-1})"                     
[36] "  vector[r] a[n];"                                                       
[37] "  matrix[r, r] R[n];"                                                    
[38] "  // likelihood of obs: p(y_t | y_t, ..., y_t-1)"                        
[39] "  vector[r]  f[n];"                                                      
[40] "  matrix[r, r] Q[n];"                                                    
[41] "  // posterior of states: p(theta_t | y_t, ..., y_t)"                    
[42] "  vector[p] m[n + 1];"                                                   
[43] "  matrix[p, p] C[n + 1];"                                                
[44] ""                                                                        
[45] "  {"                                                                     
[46] "    vector[r] err;"                                                      
[47] "    matrix[p, r] K;"                                                     
[48] "    matrix[r, r] Qinv;"                                                  
[49] "    matrix[p, p] C_tmp;"                                                 
[50] "    matrix[p, p] J;"                                                     
[51] "    "                                                                    
[52] "    // set initial states"                                               
[53] "    m[1] <- m0;"                                                         
[54] "    C[1] <- C0;"                                                         
[55] "    // loop through observations"                                        
[56] "    for (t in 1:n) {"                                                    
[57] "      // one step ahead predictive distribion of \\theta_t | y_{1:(t-1)}"
[58] "      a[t] <- g + G * m[t];"                                             
[59] "      // R[t] <- G * C[t] * G ' + W;"                                    
[60] "      R[t] <- quad_form(C[t], G ') + W;"                                 
[61] "      // one step ahead predictive distribion of y_t | y_{1:(t-1)}"      
[62] "      f[t] <- b + F * a[t];"                                             
[63] "      // Q[t] <- F * R[t] * F ' + V;"                                    
[64] "      Q[t] <- quad_form(R[t], F ') + V;      "                           
[65] "      Qinv <- inverse(Q[t]);"                                            
[66] "      // error"                                                          
[67] "      err <- y[t] - f[t];"                                               
[68] "      // Kalman gain"                                                    
[69] "      K <- R[t] * F ' * Qinv;"                                           
[70] "      // posterior distribution of \\theta_t | y_{1:t}"                  
[71] "      m[t + 1] <- a[t] + K * err;"                                       
[72] "      // matrix used in Joseph stabilized form"                          
[73] "      // C_tmp <- R[t] - K * Q[t] * K ';"                                
[74] "      // C_tmp <- R[t] - quad_form(Q[t], K ');"                          
[75] "      // C[t + 1] <- 0.5 * (C_tmp + C_tmp ');"                           
[76] "      J <- (Ip - K * F);"                                                
[77] "      C[t + 1] <- quad_form(R[t], J ') + quad_form(V, K ');"             
[78] "      // log likelihood"                                                 
[79] "      // loglik_obs[t] <- - 0.5 * (r * log(2 * pi())"                    
[80] "      // \t\t\t\t+ log_determinant(Q[t])"                                
[81] "      // \t\t\t\t+ err ' * Qinv * err);"                                 
[82] "      loglik_obs[t] <- - 0.5 * (r * log(2 * pi())"                       
[83] "      \t\t\t\t+ log_determinant(Q[t])"                                   
[84] "      \t\t\t\t+ quad_form(Qinv, err));"                                  
[85] ""                                                                        
[86] "    }"                                                                   
[87] "  }"                                                                     
[88] "  loglik <- sum(loglik_obs);"                                            
[89] "}"                                                                       
[90] "model {"                                                                 
[91] "  increment_log_prob(loglik);"                                           
[92] "}"                                                                       
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(}\hlstr{"Nile"}\hlstd{)}
\hlstd{y} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(Nile)}

\hlstd{standata} \hlkwb{<-}
    \hlkwd{within}\hlstd{(}\hlkwd{list}\hlstd{(), \{}
        \hlstd{y} \hlkwb{<-} \hlkwd{matrix}\hlstd{(y)}
        \hlstd{n} \hlkwb{<-} \hlkwd{length}\hlstd{(y)}
        \hlstd{r} \hlkwb{<-} \hlnum{1L}
        \hlstd{p} \hlkwb{<-} \hlnum{1L}
        \hlstd{b} \hlkwb{<-} \hlnum{0}
        \hlstd{g} \hlkwb{<-} \hlnum{0}
        \hlstd{F} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{)}
        \hlstd{G} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{)}
        \hlstd{m0} \hlkwb{<-} \hlkwd{array}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
        \hlstd{C0} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{1000}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{)}
        \hlstd{\})}

\hlstd{m} \hlkwb{<-} \hlkwd{stan_model}\hlstd{(}\hlstr{"stan/kalman_batch.stan"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## TRANSLATING MODEL 'kalman_batch' FROM Stan CODE TO C++ CODE NOW.
## COMPILING THE C++ CODE FOR MODEL 'kalman_batch' NOW.
\end{verbatim}
\begin{alltt}
\hlstd{ret} \hlkwb{<-} \hlkwd{sampling}\hlstd{(m,} \hlkwc{data} \hlstd{= standata)}
\end{alltt}
\begin{verbatim}
## SAMPLING FOR MODEL 'kalman_batch' NOW (CHAIN 1).
## Iteration:    1 / 2000 [  0%]  (Warmup)Iteration:  200 / 2000 [ 10%]  (Warmup)Iteration:  400 / 2000 [ 20%]  (Warmup)Iteration:  600 / 2000 [ 30%]  (Warmup)Iteration:  800 / 2000 [ 40%]  (Warmup)Iteration: 1000 / 2000 [ 50%]  (Warmup)Iteration: 1200 / 2000 [ 60%]  (Sampling)Iteration: 1400 / 2000 [ 70%]  (Sampling)Iteration: 1600 / 2000 [ 80%]  (Sampling)Iteration: 1800 / 2000 [ 90%]  (Sampling)Iteration: 2000 / 2000 [100%]  (Sampling)
## Elapsed Time: 10.0398 seconds (Warm-up)
##               10.2988 seconds (Sampling)
##               20.3385 seconds (Total)
## 
## SAMPLING FOR MODEL 'kalman_batch' NOW (CHAIN 2).
## Iteration:    1 / 2000 [  0%]  (Warmup)Iteration:  200 / 2000 [ 10%]  (Warmup)Iteration:  400 / 2000 [ 20%]  (Warmup)Iteration:  600 / 2000 [ 30%]  (Warmup)Iteration:  800 / 2000 [ 40%]  (Warmup)Iteration: 1000 / 2000 [ 50%]  (Warmup)Iteration: 1200 / 2000 [ 60%]  (Sampling)Iteration: 1400 / 2000 [ 70%]  (Sampling)Iteration: 1600 / 2000 [ 80%]  (Sampling)Iteration: 1800 / 2000 [ 90%]  (Sampling)Iteration: 2000 / 2000 [100%]  (Sampling)
## Elapsed Time: 9.94662 seconds (Warm-up)
##               8.2413 seconds (Sampling)
##               18.1879 seconds (Total)
## 
## SAMPLING FOR MODEL 'kalman_batch' NOW (CHAIN 3).
## Iteration:    1 / 2000 [  0%]  (Warmup)Iteration:  200 / 2000 [ 10%]  (Warmup)Iteration:  400 / 2000 [ 20%]  (Warmup)Iteration:  600 / 2000 [ 30%]  (Warmup)Iteration:  800 / 2000 [ 40%]  (Warmup)Iteration: 1000 / 2000 [ 50%]  (Warmup)Iteration: 1200 / 2000 [ 60%]  (Sampling)Iteration: 1400 / 2000 [ 70%]  (Sampling)Iteration: 1600 / 2000 [ 80%]  (Sampling)Iteration: 1800 / 2000 [ 90%]  (Sampling)Iteration: 2000 / 2000 [100%]  (Sampling)
## Elapsed Time: 9.67374 seconds (Warm-up)
##               9.98924 seconds (Sampling)
##               19.663 seconds (Total)
## 
## SAMPLING FOR MODEL 'kalman_batch' NOW (CHAIN 4).
## Iteration:    1 / 2000 [  0%]  (Warmup)Iteration:  200 / 2000 [ 10%]  (Warmup)Iteration:  400 / 2000 [ 20%]  (Warmup)Iteration:  600 / 2000 [ 30%]  (Warmup)Iteration:  800 / 2000 [ 40%]  (Warmup)Iteration: 1000 / 2000 [ 50%]  (Warmup)Iteration: 1200 / 2000 [ 60%]  (Sampling)Iteration: 1400 / 2000 [ 70%]  (Sampling)Iteration: 1600 / 2000 [ 80%]  (Sampling)Iteration: 1800 / 2000 [ 90%]  (Sampling)Iteration: 2000 / 2000 [100%]  (Sampling)
## Elapsed Time: 9.82326 seconds (Warm-up)
##               9.10817 seconds (Sampling)
##               18.9314 seconds (Total)
\end{verbatim}
\begin{alltt}
\hlkwd{apply}\hlstd{(}\hlkwd{extract}\hlstd{(ret,} \hlstr{"V"}\hlstd{)[[}\hlnum{1}\hlstd{]],} \hlnum{2}\hlstd{, mean)}
\end{alltt}
\begin{verbatim}
## [1] 6601
\end{verbatim}
\begin{alltt}
\hlkwd{apply}\hlstd{(}\hlkwd{extract}\hlstd{(ret,} \hlstr{"W"}\hlstd{)[[}\hlnum{1}\hlstd{]],} \hlnum{2}\hlstd{, mean)}
\end{alltt}
\begin{verbatim}
## [1] 29954
\end{verbatim}
\begin{alltt}
\hlkwd{mean}\hlstd{(}\hlkwd{extract}\hlstd{(ret,} \hlstr{"loglik"}\hlstd{)[[}\hlnum{1}\hlstd{]])}
\end{alltt}
\begin{verbatim}
## [1] -671.7
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{UK Gas}

This is a model of quarterly consumption of gas in the UK from 1960 to 1986.
This data is included with \Rlang{} as \texttt{datasets::UKGas}.
The model to be estimated is a local linear trend model and a quarterly seasonal factor model.
\begin{equation*}
  \begin{aligned}[t]
    F &=
    \begin{bmatrix}
      1 & 0 & 1 & 0 & 0 
    \end{bmatrix}
    & 
    G &=
    \begin{bmatrix}
      1 & 1 & 0 & 0 & 0  \\
      0 & 1 & -1 & -1 & -1 \\
      0 & 0 & 1 & 0 & 0 \\
      0 & 0 & 0 & 1 & 0 \\
    \end{bmatrix}
    \\
    V & = \sigma_{y}^{2} &
    W & = \diag (0, \sigma_{\beta}^{2}, \sigma_{s}^{2}, 0, 0) 
  \end{aligned}
\end{equation*}

\subsection{Industrial Production}

A linear trend model of U.S. industrial production of consumption goods, seasonally adjusted.
The data is \href{http://research.stlouisfed.org/fred2/series/IPCONGD}{IPCONGD}; also available on \href{http://www.quandl.com/FRED/IPCONGD}{Quandl}.

\begin{equation*}
  \begin{aligned}[t]
    F &=
    \begin{bmatrix}
      1 & 0 
    \end{bmatrix}
    & 
    G &=
    \begin{bmatrix}
      1 & 1 \\
      0 & 1 
    \end{bmatrix}
    \\
    V & = \sigma_{y}^{2} &
    W & = \diag (\sigma_{1}^{2}, \sigma_{2}^{2})
  \end{aligned}
\end{equation*}

\subsection{CAPM}

The state space are the betas of the stocks,
\begin{equation}
  \theta_{t} = (\beta_{1,t}, \dots, \beta_{m,t})'
\end{equation}
The system equations are,
\begin{equation}
  \begin{aligned}[t]
    F_{t} &= x_{t} I & G_{t} &= I \\
    V_{t} &= \Sigma_{\epsilon} & W_{t} &= \Sigma_{\beta}
  \end{aligned}
\end{equation}

\printbibliography{}

\end{document}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
