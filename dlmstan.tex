\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{amsmath}
\usepackage{longtable}
\usepackage[backend=biber]{biblatex}
\usepackage{hyperref}

\bibliography{default}

\newcommand{\Rlang}{\textsf{R}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}



\section{Model Terminology}

These models are called either state space models (econometrics) or
dynamic (linear) models (statistics / Bayesian).

The following defines a \emph{state space model} 
\begin{equation*}
  \begin{aligned}[t]
    y_t = f(\theta_t \vert  b_{t}, F_t, \nu_t) \\
    \theta_t = f(\theta_{t-1} \vert g_{t},  G_t, \nu_t)
  \end{aligned}
\end{equation*}

If $\theta_t$ is continuous then it is a \emph{continuous state space
model}, if $\theta_t$ is discrete then it is a \emph{discrete state
space model}.

If those equations can be written as 
\begin{equation*}
\begin{aligned}[t]
y_t = b_{t} + F_t \theta_t + \nu_t \\
\theta_t = g_{t} + G_t \theta_{t-1} + \omega_t
\end{aligned}
\end{equation*}
then the model is a \emph{Dynamic Linear Model (DLM)} (linear SSM), otherwise it is a non-linear dynamic model.
If $\nu_t$ and $\omega_t$ are normal distributions, then it is \emph{Guassian} or \emph{Normal Dynamic Linear Model} (GDLM or NDLM). 

A dynamic linear model is defined by the following set of equations,
\begin{align}
  \label{eq:2}
  y_t &= b_{t} + F_t \theta_{t-1} + \nu_t & \nu_t & \sim N(0, V_t) \\
  \label{eq:3}
  \theta_t &= g_{t} + G_t \theta_{t-1} + \omega_t & \omega_t & \sim N(0, W_t) \\
  \label{eq:4}
  \theta_0 &\sim N(m_0, C_0)
\end{align}
where equation \ref{eq:2} is the observation or measurement equation,
equation \ref{eq:3} is the system equation, 
and equation \ref{eq:4} is the initial information.
The number of variables is $r$ and the number of states is $n$.

\begin{table}
  \centering
  \begin{tabular}[c]{@{}ll@{}}
    \hline\noalign{\medskip}
    matrix & dimensions
    \\\noalign{\medskip}
    \hline\noalign{\medskip}
    $F_t$ & $r \times n$
    \\\noalign{\medskip}
    $G_t$ & $n \times n$
    \\\noalign{\medskip}
    $V_t$ & $r \times r$
    \\\noalign{\medskip}
    $W_t$ & $n \times n$
    \\\noalign{\medskip}
    $C_0$ & $n \times n$
    \\\noalign{\medskip}
    \hline
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \begin{tabular}[c]{@{}ll@{}}
    \hline\noalign{\medskip}
    vector & dimensions
    \\\noalign{\medskip}
    \hline\noalign{\medskip}
    $Y_t$ & $r$
    \\\noalign{\medskip}
    $\theta_t$ & $n$
    \\\noalign{\medskip}
    $b_t$ & $r$
    \\\noalign{\medskip}
    $\nu_t$ & $r$
    \\\noalign{\medskip}
    $g_t$ & $n$\
    \\\noalign{\medskip}
    $\omega_t$ & $n$
    \\\noalign{\medskip}
    $m_0$ & $n$
    \\\noalign{\medskip}
    \hline
  \end{tabular}
\end{table}


\subsection{Filtering Equations}

See \textcite[Chapter 2.7, p. 53]{PetrisPetroneEtAl2009} and \textcite[Chapter 4]{WestHarrison1997} for proofs.

Assume the posterior distribution at $t-1$. 
Let $\theta_{t-1} | y_{1:t-1} \sim N(m_{t-1}, C_{t-1})$.
The one step ahead predictive distribution of $\theta_{t-1}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$,
\begin{align}
  a_{t} &= \E(\theta_{t} | y_{1:t-1}) &&= g_{t} + G_{t} m_{t-1} \\
  R_{t} &= \Var(\theta_{t} | y_{1:t-1}) &&= G_{t} C_{t-1} G'_{t} + W_{t}
\end{align}
The one step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$,
\begin{align}
  f_{t} &= \E(Y_{t} | y_{1:t-1}) &&= b_{t} + F_{t} a_{t} \\
  Q_{t} &= \Var(Y_{t} | y_{1:t-1}) &&= F_{t} R_{t} F'_{t} + V_{t}
\end{align}
The filtered distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
\begin{align}
  \label{eq:5}
  m_{t} &= \E(\theta_{t} | y_{1:t}) &&= a_{t} + R_{t} F'_{t} Q_{t}^{-1} e_{t} \\
  &&& = a_{t} + K_{t} e_{t} \\
  \label{eq:6}
  C_{t} &= \Var(\theta_{t} | y_{1:t}) &&= R_{t} - R_{t} F'_{t} Q_{t}^{-1} F_{t} R_{t} \\
  &&& = R_{t} - K_{t} Q_{t} K'_{t} \\
  \label{eq:1}  
  &&& = (I_{n} - K_{t} F_{t}) R_{t} (I_{n} - K_{t} F_{t})' + K_{t} V_{t} K_{t}' \\
  e_{t} &&&= y_{t} - f_{t} \\
  K_{t} &&&= R_{t} F_{t}' Q_{t}^{-1}
\end{align}
where $e_{t}$ is the one step forecast error and $K_{t}$ is the Kalman gain (adaptative coefficient).
Equation \ref{eq:1} is the ``Joseph stablized form'' \parencite[3]{Tusell2011}.

\begin{table}
  \centering
  \begin{tabular}[]{ll}
    \hline
    variable & dim \\
    \hline
    $a_{t}$ & $n$ \tabularnewline
    $R_{t}$ & $n, n$ \tabularnewline
    $f_{t}$ & $r$ \tabularnewline
    $Q_{t}$ & $r, r$ \tabularnewline
    $m_{t}$ & $n$ \tabularnewline
    $C_{t}$ & $n, n$ \tabularnewline
    $e_{t}$ & $r$ \tabularnewline
    $K_{t}$ & $n, r$ 
  \end{tabular}
  \label{Dimensions of variables in the filtering equations.}
\end{table}

A few key identies \parencite[106-107]{WestHarrison1997} are,
\begin{align}
  K_{t} &= R_{t} F'_{t} Q_{t}^{-1} = C_{t} F'_{t} V_{t}^{-1} \\
  C_{t} &= R_{t}^{-1} - K_{t} Q_{t} K_{t} = R_{t}(I_{n} - F_{t} K_{t}') \\
  C_{t}^{-1} &= R_{t}^{-1} + F_{t}' V_{t}^{-1} F_{t} \\
  Q_{t} &= (I_{r} - F_{t} K_{t})^{-1} V_{t} \\
  F_{t} K_{t} &= I_{r} - V_{t} Q_{t}^{-1}
\end{align}
% Replace $a_{t} = G_{t} m_{t - 1} + \E(\omega_{t})$ and 

\subsection{Missing Values}

If all values in $t$ are missing, replace the filter steps (equations \ref{eq:5} and \ref{eq:6}) with,
\begin{align}
  m_{t} = a_{t} \\
  C_{t} = R_{t} 
\end{align}

Suppose some, but not all variables in time $t$ are missing.
Let $n_{t} \in [0, r]$ is the number of non-missing values in each time period, $n_{t} = \sum_{j = 1}^{r} (y_{j,t} \neq \emptyset)$.
If some are missing (let $r > r_{t} > 0$ be observed), let $M$ be a $r_{t} \times r$ selection matrix and define,
\begin{align}
  y^{*}_{t} &= M_{t} y_{t} \\
  F^{*}_{t} &= M_{t} F_{t} \\
  V^{*}_{t} &= M_{t} V_{t} M_{t}'
\end{align}
and replace $y_{t}$, $F_{t}$ and $V_{t}$ in the filtering equations with those.
Note that the smoothing (section \ref{sec:smoothing}) and backward sampling (section \ref{sec:backward-sampling}) algorithms do not depend on $F$, $V$ and $y$ and thus do not need to be altered when there are missing values.


\subsection{Likelihood}
\label{sec:likelihood}

If there are no missing values, then the log likelihood is,
\begin{equation}
  L(y_{1:T}) =  - \frac{n T}{2} \log (2 \pi) - \frac{1}{2} \sum_{t=1}^{T}
  \left(
    \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
\end{equation}
If there are missing values,
\begin{equation}
  L(y_{1:T}) = 
  -\frac{1}{2} \sum_{t=1}^{T} 
  (n_{t} > 0) 
  \left(
    n_{t} \log (2 \pi)
    + \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
\end{equation}

See \textcite[Chapter 5, p. 57]{KoopmanShephardDoornik2008}.


\subsection{Smoothing}
\label{sec:smoothing}

If $\theta_{t+1} | y_{1:T} \sim N(s_{t+1}, S_{t+1})$, then $\theta_{t} | y_{1:T} \sim N(s_{t}, S_{t})$ where
\begin{align}
  s_{t} &= \E(\theta_{t} | y_{1:T}) &&= m_{t} + C_{t} G'_{t+1} R_{t+1}^{-1}(s_{t+1} - a_{t+1}) \\
  S_{t} &= \Var(\theta_{t} | y_{1:T}) &&= C_{t} - C_{t} G'_{t+1} R^{-1}_{t+1} (R_{t+1} - S_{t+1}) R^{-1}_{t+1} G_{t+1} C_{t}
\end{align}
Note that $s_{T} = m_{T}$ and $S_{T} = C_{T}$.
See \textcite[Prop 2.4, p. 61]{PetrisPetroneEtAl2009} for a proof.


\subsection{Backward Sampling}
\label{sec:backward-sample}

Supposing that $m_{1:T}$, $C_{1:T}$, $a_{1:T}$ and $R_{1:T}$ have been calculated by the filter,%
\footnote{No additional adjustment for intercepts is required because they are already incorporated in $a_{t}$}
To draw $\theta_{1:T} | y_{1:T}$,

\begin{enumerate}
\item From the Kalman filter, $\theta_{T} | y_{1:T} \sim N(m_{T}, C_{T})$
\item For $t = (T-1), \dots, 0$, $\theta_{t} | y_{1:T} \sim N(h_{t}, H_{t})$ where
  \begin{align}
    h_{t} &= m_{t} + C_{t} G'_{t + 1} R_{t+1}^{-1}(\theta_{t+1} - a_{t+1}) \\
    H_{t} &= C_{t} - C_{t} G'_{t} R_{t+1}^{-1} G_{t+1} C_{t}
  \end{align}
\end{enumerate}
See \textcite[Chapter 4.4.1, p. 161]{PetrisPetroneEtAl2009} for more details.

\section{Sequential Estimation}

First, consider the case in which all $V_{t}$ are diagonal.
The vector series $y_{1}, \dots, y_{T}$ is treated as a scalar series
\begin{equation}
  y_{1,1}, \dots, y_{1,r}, y_{2,1}, \dots, y_{T,r}
\end{equation}

The prior distribution of the state is $\theta_{t} | y_{1:(t-1)} \sim N(a_{t}, R_{t})$,
\begin{align}
  a_{t} &= \E(\theta_{t} | y_{1:t-1}) &&= g_{t} + G_{t} m_{t-1} \\
  R_{t} &= \Var(\theta_{t} | y_{1:t-1}) &&= G_{t} C_{t-1} G'_{t} + W_{t}
\end{align}

For each variable, $i = 1, \dots, r$.
The prediction equation is $y_{t,i} | y_{1:t-1}, y_{t,j|j < i} \sim N(f_{t,i}, Q_{t,i})$.
Note that $f_{t,i}$ and $Q_{t,i}$ are scalars.
\begin{align}
  f_{t,i} &= \E(Y_{t,i} | y_{1:t-1}, y_{t,j | j < i}) &&= b_{t,i} + F_{t,i} m_{t,i-1} \\
  Q_{t,i} &= \Var(Y_{t,i} | y_{1:t-1}, y_{t, j | j < i}) &&= F_{t,i} C_{t,i-1} F'_{t,i} + v_{t,i}
\end{align}
For the first variable, let $m_{t,0} = a_{t}$ and $C_{t,0} = R_{t}$.
The filtered distribution of the latent state is $\theta_{t} | y_{1:t-1}, y_{t,j|j < i} \sim N(m_{t,i}, C_{t,i})$,
\begin{align}
  m_{t,i} &= \E(\theta_{t} | y_{1:t-1}, y_{t, j | j \leq i}) &&= m_{t,i-1} + C_{t,i-1} F'_{t,i} Q_{t,i}^{-1} e_{t,i} \\
  C_{t,i} &= \Var(\theta_{t} | y_{1:t-1}, y_{t, j|j \leq i}) &&= C_{t,i-1} - C_{t,i-1} F'_{t,i} Q_{t,i}^{-1} F_{t,i} C_{t,i-1} \\
  e_{t,i} &&&= y_{t,i} - f_{t,i} \\
  K_{t,i} &&&= C_{t,i-i} F_{t,i}' Q_{t,i}^{-1}
\end{align}
Define $m_{t} = m_{t,r}$ and $C_{t} = C_{t,r}$.
If $y_{t,i}$ is missing, then 
\begin{align}
  m_{t,i} &= m_{t,i-1} \\
  C_{t,i} &= C_{t,i-1}
\end{align}

The likelihood in the sequential case is
\begin{equation}
  L(y_{1:T}) = -\frac{1}{2} \sum_{t=1}^{T} \sum_{i=1}^{r} (y_{i,j} \neq \emptyset)
  \left( 
    \log (2 \pi) + \log |Q_{t,i}| + \frac{e_{t,i}^{2}}{Q_{t,i}} 
  \right)
\end{equation}

If $V_{t}$ is not diagonal, then diagonalize it with the Cholesky decomposition of $V_{t}$, such that $V_{t} = L_{t} D_{t} L'_{t}$ where $D_{t}$ is diagonal, and $L_{t}$ is lower triangular.
\begin{align}
  y_{t}^{*} &= L_{t}^{-1} y_{t} \\
  F_{t}^{*} &= L_{t}^{-1} F_{t} \\
  b_{t}^{*} &= L_{t}^{-1} b_{t} \\
  \epsilon^{*} & = L_{t}^{-1} \epsilon_{t}  \sim N(0, D_{t})
\end{align}

%% \subsection{Discounting}
%% \label{sec:smoothing-1}

%% See \textcite[Chapter 6.3]{WestHarrison1997}

%% Let $\Delta$ be covariance matrix, with the entries in the diagonal between 0 and 1.

%% \begin{itemize}
%% \item Component discounting (West and Harrison)
%%   \begin{equation*}
%%     W_{t} = \frac{1 - \delta}{\delta} G_{t} C_{t - 1} G_{t}'
%%   \end{equation*}
%%   Let $R_{t} = \Delta^{-1} P_{t}$ and $R_{t} = P_{t} + W_{t}$ where $P_{t} = G_{t} C_{t-1} G_{t}'$,
%%   \begin{equation*}
%%     W_{t} = (\Delta^{-1} - I) P_{t} 
%%   \end{equation*}
%% \item Let $R_{t} = \Delta G_{t} C_{t-1} G'_{t} \Delta$ (p. 202)
%% \item Let $R_{t} = G_{t} \Delta C_{t-1} \Delta G'_{t}$ (p. 202)
%% \end{itemize}


\subsection{Univariate Local Level Model}
\label{sec:local-level-model}

For the univariate local level model, with $r = n = 1$, $F_{t} = G_{t} = 1$,
and $g_{t} = b_{t} = 0$, the calculations can be simplified.

One step ahead predictive distribution of $\theta_{t-1}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$
\begin{align*}
  a_{t} &= E(\theta_{t} | y_{1:t-1}) = m_{t-1} \\
  R_{t} &= Var(\theta_{t} | y_{1:t-1}) = C_{t-1} + W_{t}
\end{align*}

The One step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$
\begin{align*}
  f_{t} &= E(Y_{t} | y_{1:t-1}) = a_{t} = m_{t-1} \\
  Q_{t} &= Var(Y_{t} | y_{1:t-1}) = R_{t} + V_{t} = C_{t-1} + W_{t} + V_{t}
\end{align*}

The posterior distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
\begin{align*}
  f_{t} &= E(\theta_{t} | y_{1:t}) = a_{t} + R_{t} Q_{t}^{-1} e_{t} = m_{t-1} + \frac{C_{t-1} + W_{t}}{C_{t-1} + W_{t} + V_{t}} e_{t} \\
  Q_{t} &= Var(\theta_{t} | y_{1:t}) = R_{t} - \frac{(C_{t-1} + W_{t})^{2}}{C_{t-1} + W_{t} + V_{t}}
\end{align*}
where $e_{t} = Y_{t} - f_{t}$. The Kalman gain is defined as $K_{t} = R_{t} / Q_{t}$.



\section{Notes}

\begin{itemize}
\item Use discounting
  \begin{itemize}
  \item Use a beta rectangular distribution
  \item Beta distribution with priors over $\alpha$  and $\beta$
  \end{itemize}
\end{itemize}



\section{Examples}


\subsection{Nile}

The data is included with \Rlang{} as \texttt{datasets::Nile}.

\begin{enumerate}
\item Standard
\item With missing
\item With intervention
\end{enumerate}


\subsection{UK Gas}

Example from 

This is a model of quarterly consumption of gas in the UK from 1960 to 1986.
This data is included with \Rlang{} as \texttt{datasets::UKGas}.
The model to be estimated is a local linear trend model and a quarterly seasonal factor model.
\begin{equation*}
  \begin{aligned}[t]
    F &=
    \begin{bmatrix}
      1 & 0 & 1 & 0 & 0 
    \end{bmatrix}
    & 
    G &=
    \begin{bmatrix}
      1 & 1 & 0 & 0 & 0  \\
      0 & 1 & -1 & -1 & -1 \\
      0 & 0 & 1 & 0 & 0 \\
      0 & 0 & 0 & 1 & 0 \\
    \end{bmatrix}
    \\
    V & = \sigma_{y}^{2} &
    W & = \diag (0, \sigma_{\beta}^{2}, \sigma_{s}^{2}, 0, 0) 
  \end{aligned}
\end{equation*}

\subsection{Industrial Production}

A linear trend model of U.S. industrial production of consumption goods, seasonally adjusted.
The data is \href{http://research.stlouisfed.org/fred2/series/IPCONGD}{IPCONGD}; also available on \href{http://www.quandl.com/FRED/IPCONGD}{Quandl}.

\begin{equation*}
  \begin{aligned}[t]
    F &=
    \begin{bmatrix}
      1 & 0 
    \end{bmatrix}
    & 
    G &=
    \begin{bmatrix}
      1 & 1 \\
      0 & 1 
    \end{bmatrix}
    \\
    V & = \sigma_{y}^{2} &
    W & = \diag (\sigma_{1}^{2}, \sigma_{2}^{2})
  \end{aligned}
\end{equation*}

\subsection{CAPM}

The state space are the betas of the stocks,
\begin{equation}
  \theta_{t} = (\beta_{1,t}, \dots, \beta_{m,t})'
\end{equation}
The system equations are,
\begin{equation}
  \begin{aligned}[t]
    F_{t} &= x_{t} I & G_{t} &= I \\
    V_{t} &= \Sigma_{\epsilon} & W_{t} &= \Sigma_{\beta}
  \end{aligned}
\end{equation}



\section{Durbin and Koopmans}

\begin{equation*}
\begin{aligned}[t]
  y_t &= Z_t \alpha_{t} + \varepsilon_t & \varepsilon_t & \sim N(0, H_t) \\
  \alpha_{t+1} &= T_t \alpha_{t} + R_t \eta_t & \eta_t & \sim N(0, Q_t) \\
  \alpha_1 &\sim N(a_1, P_1)
\end{aligned}
\end{equation*}
with dimensions $r$ (number of disturbances), $p$ (number of variables)
and $m$ (number of states).

Matrices,
\begin{longtable}[c]{@{}ll@{}}
\hline\noalign{\medskip}
matrix & dimensions
\\\noalign{\medskip}
\hline\noalign{\medskip}
$Z_t$ & $p \times m$
\\\noalign{\medskip}
$T_t$ & $m \times m$
\\\noalign{\medskip}
$H_t$ & $p \times p$
\\\noalign{\medskip}
$Q_t$ & $r \times r$
\\\noalign{\medskip}
$R_t$ & $m \times r$
\\\noalign{\medskip}
$P_1$ & $m \times m$
\\\noalign{\medskip}
\hline
\end{longtable}
and vectors,
\begin{longtable}[c]{@{}ll@{}}
\hline\noalign{\medskip}
vector & dimensions
\\\noalign{\medskip}
\hline\noalign{\medskip}
$y_t$ & $p \times 1$
\\\noalign{\medskip}
$\alpha_t$ & $m \times 1$
\\\noalign{\medskip}
$\varepsilon_t$ & $p \times 1$
\\\noalign{\medskip}
$\eta_t$ & $r \times 1$
\\\noalign{\medskip}
$a_1$ & $m \times 1$
\\\noalign{\medskip}
\hline
\end{longtable}

This is initialized with the filtered states.



\printbibliography{}

\end{document}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
